---
title: "Comprehensive R Course - Descriptive and Predictive Analysis"
author: "Basim Alsaedi"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
---

# Introduction

## Purpose of this Book

-   **To Master Descriptive Statistics**: Teach readers how to summarize and describe essential features of data effectively, enabling a deeper understanding of datasets.
-   **To Equip Readers with Hypothesis Testing Skills**: Provide a solid foundation in conducting and interpreting various statistical tests, including Chi-Square, t-tests, z-tests, and non-parametric tests, crucial for making informed decisions based on data.
-   **To Introduce ANOVA and F-tests**: Guide readers through the process of analyzing variance among group means with the ANOVA technique and explain the importance of F-tests in comparing statistical models.
-   **To Explore Regression Analysis**: Offer comprehensive insights into Ordinary Least Squares (OLS) and Multiple Linear Regression (MLR), essential tools for modeling and predicting continuous outcomes.
-   **To Understand Categorical Data Analysis**: Provide the basics of analyzing categorical data using simple logistic regression, essential for situations where response variables are categorical.

## Overview of R and the RStudio Interface

R is a programming language and free software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing. It is widely used among statisticians and data miners for developing statistical software and data analysis.

To install R, visit CRAN (Comprehensive R Archive Network) at [cran.r-project.org](https://cran.r-project.org/), select your operating system, and follow the provided installation instructions. After installing R, download RStudio, a popular IDE for R, from [rstudio.com](https://www.rstudio.com/products/rstudio/download/), choosing the appropriate installer for your OS.

R is the actual programming language used for statistical analysis and graphics, while RStudio provides an integrated development environment to write R code and visualize its outputs more conveniently. RStudio enhances R's user experience with features like syntax highlighting, direct code execution, and graphical representation but requires R to be installed first.

### Navigating RStudio: Panels, Scripts, Console, and Environment

RStudio is an Integrated Development Environment (IDE) for R. It includes a console, a syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging, and workspace management.

Here are demonstrations on how R works:

```{r navigation-RStudio1, echo=TRUE, message=FALSE, warning=FALSE}
# Simple code to demonstrate the console output with the built-in 'cars' dataset
summary(cars)
```

```{r navigation-RStudio2, echo=TRUE, message=FALSE, warning=FALSE}
# Basic data visualization in the Plots panel
plot(cars$speed, cars$dist, main = "Stopping Distances vs Speed")
```

```{r navigation-RStudio3, echo=TRUE, message=FALSE, warning=FALSE, eval=TRUE}
# Using the Help panel to access documentation
?summary
```

```{r navigation-RStudio4, echo=TRUE, message=FALSE, warning=FALSE}

# a simple function and viewing it in the Source panel
# ideal for reusing the same code
my_mean_function <- function(x) {
  sum(x) / length(x)
}
my_mean_function(cars$speed)

# TODO: add screenshots and diagrams to visually guide through the RStudio interface.
```

Each of these code chunks illustrates a different aspect of RStudio:

1.  **Basic data visualization**: Introduces us to R's plotting capabilities, which will display in the Plots panel of RStudio.
2.  **Accessing documentation**: Shows how to use the Help system in RStudio, demonstrating how to find information on specific functions.
3.  **Writing a function**: Encourages users to practice scripting in the Source panel and understand the concept of function creation in R.

### RStudio Projects and Workspace Management

RStudio projects make managing your R work much easier. They allow you to encapsulate all of the materials for a single analysis in one place, simplifying paths and workspace management.

```{r RStudio-projects, eval=FALSE}
# This is only a demonstration.
# To create a new project:
# - Click on 'File' -> 'New Project...'
# - Follow the prompts to set up a new project
```

### Installing and Managing Packages

R’s functionality is divided into a number of packages, which are free libraries of code written by R's active user community. This is how we install a package, and then load it into our environment, so as to use it throughout our session:

```{r install-packages, eval=FALSE}
# Example code to install and load a package
install.packages("tidyverse")
library(tidyverse)
```

![alt text](image.png)

## Importance of Data Analysis with R

### Role of Data Analysis in Modern Industries

Data analysis isn’t just crunching numbers; it’s about telling stories and making informed decisions. In modern industries, it guides everything from predicting market trends to optimizing supply chains. With R, you're not just learning a programming language, but gaining a key to unlock these data-driven narratives.

### Case Studies of R in Action

Imagine a company figuring out what makes a marketing campaign successful or a scientist discovering a groundbreaking way to save endangered species—all with the help of R. These real-world successes aren't just stories; they're blueprints for what you can accomplish with data analysis in R.

### R's Ecosystem and Community

Joining R is like moving into a friendly neighborhood. There’s always someone to lend you a hand—whether it's through forums, user groups, or conferences. And with an ever-growing collection of packages, R is like a toolbox that's constantly being filled with new, shiny tools.

### Future Trends in Data Analysis with R

Data analysis with R is like having a crystal ball; it helps predict future trends, from advancements in AI and machine learning to big data's expanding role. As you dive into R, you’re not just keeping up—you’re riding the wave of the data revolution.

------------------------------------------------------------------------

## Exercises

**1. Get to know RStudio.** - Open RStudio and identify each of the four main panels.

**2. Create your first RStudio project.** - Use the RStudio interface to start a new project in a new directory.

**3. Install and load a package.** - Install the `dplyr` and `tidyverse` packages and load them using `library()`.

``` r
# TODO: Insert Exercise chunk with error messages and solutions.
```

<!--chapter:end:index.Rmd-->

---
title: "Code Challenge 2: Data Structures and Data Handling"
output: 
  learnr::tutorial:
    progressive: true
runtime: shiny_prerendered
---

# R Programming Fundamentals (Light Focus)

## Basics of R Syntax and Functions

Understanding the basics of R syntax and how functions work is essential for any aspiring data analyst. Here’s a breakdown to help you get started.

## Variables and Data Types
In R, variables are used to store data which can be of various types: numeric, integer, character (string), and logical (boolean). Setting up variables correctly is crucial for managing and manipulating your data effectively.

```r
# Example of defining different types of variables
a <- 5.5  # Numeric
b <- 3    # Integer
c <- "Hello, world!"  # Character
d <- TRUE  # Logical
```

## Control Structures: Loops and Conditional Statements
Control structures in R, such as loops and conditional statements, control the flow of execution of the script. `if-else` statements make decisions, while loops (`for`, `while`) repeat actions, which is very useful for automating repetitive tasks.

```r
# Example of an if-else statement
if (a > b) {
  print("a is greater than b")
} else {
  print("b is equal to or greater than a")
}

# Example of a for loop
for(i in 1:5) {
  print(paste("Iteration", i))
}
```
```r
# TODO: Demonstration
```

## Writing and Using Functions
Functions are blocks of code that you can reuse. They are defined using the `function()` keyword and can return a value using the `return()` function, although it's optional as R automatically returns the last expression evaluated.

```r
# Example of writing a simple function
my_sum <- function(x, y) {
  sum <- x + y
  return(sum)
}

# Using the function
my_sum(5, 3)
```
```r
# TODO: Demonstration
```
## Debugging and Error Handling
Debugging is an essential skill in programming. R provides several tools for debugging, such as `browser()`, `traceback()`, and `debug()`. Error handling can be performed using `try()`, `tryCatch()`, and `stop()` functions to manage exceptions and maintain the flow of execution.

```r
# Example of simple error handling
result <- try(log(-1), silent = TRUE)
if(inherits(result, "try-error")) {
  print("Error in log(-1) : NaN produced")
} else {
  print(result)
}
```
```r
# TODO: Demonstration
```

## Code Exercise 1

**Exercise 1:** Create variables of each type and print them.
```r
# TODO: Insert Exercise chunk with error messages and solutions.
```
**Exercise 2:** Write an `if-else` statement that checks if a number is even or odd.
```r
# TODO: Insert Exercise chunk with error messages and solutions.
```
**Exercise 3:** Write a for loop that calculates the factorial of a number.
```r
# TODO: Insert Exercise chunk with error messages and solutions.
```
**Exercise 4:** Write a function that takes a vector and returns its mean, handling any NA values.
```r
# TODO: Insert Exercise chunk with error messages and solutions.
```
**Exercise 5:** Implement error handling for a division function that prints an error message when trying to divide by zero.
```r
# TODO: Insert Exercise chunk with error messages and solutions.
```

These exercises are designed to solidify your understanding of R's fundamental programming concepts and get you hands-on experience writing and debugging R code. They serve as a practical way to apply what you've learned and prepare you for more complex data analysis tasks.


# Efficient Programming Practices

Adopting efficient programming practices is essential for writing clean, fast, and reliable R code. This section covers key aspects that every R programmer should know to enhance their coding efficiency.

## Code Organization and Readability
Organizing your code properly and ensuring it is easy to read are fundamental for both solo and collaborative projects. This involves:

- **Commenting generously**: Write comments that explain the "why" behind the code, not just the "what".
- **Consistent naming conventions**: Use clear and descriptive names for variables and functions. Stick to a naming convention such as snake_case or camelCase.
- **Logical structuring**: Group related code blocks together, and separate sections by functionality.
- **Modularity**: Break down code into reusable functions to reduce redundancy and improve maintainability.

```r
# Example of well-organized code
calculate_mean <- function(numbers) {
  # This function calculates the mean of a numeric vector, handling NA values
  if(length(numbers) == 0) {
    stop("Input vector is empty")
  }
  sum(numbers, na.rm = TRUE) / length(na.omit(numbers))
}
```

## Introduction to R's Vectorized Operations
Vectorized operations are one of the most powerful features of R, allowing you to operate on entire vectors of data without the need for explicit loops. This not only makes the code cleaner but also significantly faster.

- **Use vectorized functions**: Functions like `sum()`, `mean()`, `min()`, and `max()` are inherently vectorized.
- **Avoid loops when possible**: Replace loops with vectorized operations to leverage R's internal optimizations.

```r
# Non-vectorized vs vectorized approach
# Calculate the square of each number in a vector
numbers <- 1:10

# Non-vectorized approach
squares_loop <- vector("numeric", length(numbers))
for(i in seq_along(numbers)) {
  squares_loop[i] <- numbers[i]^2
}

# Vectorized approach
squares_vectorized <- numbers^2
```

## Best Practices for Speed and Performance
Improving the speed and performance of your R scripts can be crucial, especially when dealing with large datasets.

- **Pre-allocate vectors**: Growing a vector inside a loop can be costly. Pre-allocate the vector to its full required length before the loop starts.
- **Use efficient data structures**: Utilize data structures like `data.table` or `tibble` for large data sets.
- **Simplify expressions**: Reduce the complexity of your calculations by simplifying expressions and removing redundant calculations.

```r
# Pre-allocating a vector
results <- vector("numeric", length = 100)
for(i in 1:100) {
  results[i] <- i^2
}
```

## Using R Profiler for Code Optimization
R provides profiling tools to help identify bottlenecks in your code. `Rprof()` starts the profiler, and `summaryRprof()` helps analyze the profiler's output.

- **Identify slow functions**: Use the profiler to see which functions are taking up most of the execution time.
- **Optimize those functions**: Focus your optimization efforts on those parts of the code that consume the most time.

```r
# Example of using R profiler
Rprof()
source("your_script.R")  # replace 'your_script.R' with the path to your R script
Rprof(NULL)
summaryRprof()
```

```{r setup, include=FALSE}
library(learnr)
library(gradethis)
```

## Code Exercises

Exercise 1: Refactor a provided piece of poorly organized R code to improve its readability and structure.

```{r refactor, exercise=TRUE, exercise.eval=TRUE}
# Initial code: Computes the mean of positive numbers in a vector
numbers <- c(1, -1, 2, -2, 3, -3)
result <- 0
count <- 0
for (i in numbers) {
  if (i > 0) {
    result <- result + i
    count <- count + 1
  }
}
mean <- result / count

# Refactor the above code to improve readability and structure.
# Hint: Use vectorized operations and create a function to calculate the mean.
```

```{r refactor-check}
grade_code(
  correct_code = {
    positive_mean <- function(numbers) {
      positive_numbers <- numbers[numbers > 0]
      mean(positive_numbers)
    }
    positive_mean(c(1, -1, 2, -2, 3, -3))
  }
)
```

Exercise 2: Convert a loop that calculates the logarithm of each element in a large vector into a vectorized expression.

```{r vectorize, exercise=TRUE, exercise.eval=TRUE}
# Loop version: Calculate the logarithm base 10 of each element
numbers <- seq(1, 1000, by = 1)
log_values <- vector("numeric", length(numbers))
for (i in seq_along(numbers)) {
  log_values[i] <- log10(numbers[i])
}

# Rewrite the above loop as a vectorized expression.
# Hint: Use a vectorized log10() function.
```

```{r vectorize-check}
grade_code(
  correct_code = {
    log_values <- log10(seq(1, 1000, by = 1))
  }
)
```

Exercise 3: Write a script that calculates the mean of 1 million randomly generated numbers, first using a loop and then using a vectorized function. Compare the time taken for both methods.

```{r mean-calculation, exercise=TRUE, exercise.eval=TRUE}
# Generating random numbers
set.seed(123)
random_numbers <- runif(1e6)

# Loop version
system.time({
  total <- 0
  for (i in random_numbers) {
    total <- total + i
  }
  mean_loop <- total / length(random_numbers)
})

# Calculate the mean using a vectorized method and compare the time.
# Hint: Use the mean() function.
```

```{r mean-calculation-check}
grade_code(
  correct_code = {
    system.time({
      mean_vectorized <- mean(random_numbers)
    })
  }
)
```

Exercise 4: Use `Rprof()` to profile the script from Exercise 3 and identify any potential bottlenecks.

```{r profiling, exercise=TRUE, exercise.eval=TRUE}
# Assume vectorized mean calculation from Exercise 3 is saved in `mean_vectorized`
Rprof(tmp <- tempfile())
mean_vectorized <- mean(random_numbers)
Rprof(NULL)

# Analyze the profiling data to identify potential bottlenecks.
# Hint: Use summaryRprof() to see the profiling summary.
summaryRprof(tmp)
```

```{r profiling-check}
grade_code(
  correct_code = {
    Rprof(tmp <- tempfile())
    mean_vectorized <- mean(random_numbers)
    Rprof(NULL)
    summaryRprof(tmp)
  }
)
```

<!--chapter:end:01-R-Fundamentals.Rmd-->

# Data Structures and Data Handling in R (Light Focus)

## Overview of Vectors, Matrices, Lists, and Data Frames

Understanding and effectively manipulating basic data structures in R is foundational for all subsequent data manipulation and analysis tasks. This section will guide you through creating and working with vectors, matrices, lists, and data frames.

## Creating and Manipulating Vectors

Vectors are the simplest and most common data structure in R. They store an ordered collection of elements of the same type. Understanding vectors is crucial as they form the building blocks for more complex data structures.

```r
# Creating a numeric vector
numbers <- c(1, 2, 3, 4, 5)

# Demonstrating vector concatenation by adding an element
numbers <- c(numbers, 6)

# Accessing specific elements in a vector using their index
second_element <- numbers[2]  # Accesses the second element in the vector

# Creating a vector for temperature conversion
temperature_c <- c(23, 21, 24, 22, 20)
temperature_f <- temperature_c * 9 / 5 + 32  # Convert to Fahrenheit
print(temperature_f)
```

## Operations with Matrices and Arrays

Matrices are vectors with a dimension attribute. They are useful for handling two-dimensional data. Arrays extend this concept to multiple dimensions.

```r
# Creating a matrix using the matrix function
matrix_data <- matrix(1:9, nrow=3, ncol=3)

# Accessing a specific element (row 1, column 2)
element <- matrix_data[1, 2]

# Transposing a matrix (switch rows and columns)
transposed <- t(matrix_data)

# Multiplying first row elements by 2 to demonstrate manipulation
matrix_data[1, ] <- matrix_data[1, ] * 2
print(matrix_data)
print(transposed)
```

## Understanding Lists and Their Uses

Lists in R are a generic vector capable of holding elements of different types. This makes them extremely versatile for datasets that require a mix of numeric, character, and logical types.

```r
# Creating a list with mixed types
my_list <- list(name="John", scores=c(95, 82, 90), passed=TRUE)

# Accessing elements within a list using the $ operator
scores <- my_list$scores
print(scores)

# Adding a new element to the list to show its flexibility
my_list$address <- "123 Elm St"
print(my_list)
```

## Data Frames for Tabular Data

Data frames are used extensively in R for storing tabular data. They resemble a table in a relational database or an Excel spreadsheet, with each column potentially holding a different type of data.

```r
# Creating a data frame
students <- data.frame(
  name = c("Alice", "Bob", "Carol"),
  score = c(88, 93, 85),
  pass = c(TRUE, TRUE, FALSE)
)

# Accessing a specific column using the $ operator
names <- students$name

# Adding a new column to indicate if the student scored an A (score > 90)
students$grade <- ifelse(students$score > 90, "A", "B")
print(students)
```

## Code Exercises

**Exercise 1:** Create a numeric vector representing the temperatures of the days of the week and convert them to Fahrenheit. Print the results.

```r
days_temps_c <- c(16, 18, 15, 17, 16, 19, 18)
days_temps_f <- days_temps_c * 9 / 5 + 32
print(days_temps_f)
```
```r
# TODO: Insert Exercise chunk with error messages and solutions.
```
**Exercise 2:** Create a 3x3 matrix with numbers from 1 to 9, then multiply the second row by 2. Print the modified matrix.

```r
matrix_3x3 <- matrix(1:9, nrow=3)
matrix_3x3[2, ] <- matrix_3x3[2, ] * 2
print(matrix_3x3)
```
```r
# TODO: Insert Exercise chunk with error messages and solutions.
```
**Exercise 3:** Create a list containing a vector of your favorite fruits, your age, and whether you like biking. Print each element.

```r
personal_info <- list(fruits = c("Apple", "Banana", "Cherry"), age = 30, likes_biking = TRUE)
print(personal_info$fruits)
print(personal_info$age)
print(personal_info$likes_biking)
```
```r
# TODO: Insert Exercise chunk with error messages and solutions.
```
**Exercise 4:** Create a data frame representing three students with fields for name, age, and favorite color. Then add a column to indicate if their favorite color is blue. Print the data frame.

```r
students_df <- data.frame(
  name = c("Alice", "Bob", "Carol"),
  age = c(20, 22, 19),
  favorite_color = c("blue", "red", "blue")
)

# Adding a column to indicate if their favorite color is blue
students_df$is_blue <- students_df$favorite_color == "blue"
print(students_df)
```
```r
# TODO: Insert Exercise chunk with error messages and solutions.
```

These exercises are designed to build your confidence in working with R’s fundamental data structures, allowing you to effectively manage and analyze data. This hands-on approach helps cement the concepts by applying them in practical scenarios.


# Data Handling Techniques with dplyr

The `dplyr` package in R is a powerful tool for data manipulation, providing a coherent set of verbs that help you solve the most common data manipulation challenges. This section will cover how to use these verbs to select, filter, arrange, mutate, summarize, join, and group your data.

## Selecting, Filtering, and Arranging Data

`dplyr` makes it easy to select specific columns of data, filter rows based on conditions, and arrange data in ascending or descending order.

## Selecting Data

Use `select()` to choose a subset of columns to keep.

```r
library(dplyr)

# Sample data frame
data <- data.frame(
  name = c("Alice", "Bob", "Carol", "David"),
  age = c(25, 30, 19, 22),
  salary = c(50000, 54000, 32000, 45000)
)

# Selecting only the name and age columns
selected_data <- select(data, name, age)
print(selected_data)
```

## Filtering Data

`filter()` is used to retrieve rows that meet certain conditions.

```r
# Filtering to find only those over 25 years old
filtered_data <- filter(data, age > 25)
print(filtered_data)
```

## Arranging Data

`arrange()` is used to reorder rows of a data frame.

```r
# Arranging data by age in ascending order
arranged_data <- arrange(data, age)
print(arranged_data)
```

## Mutating and Summarizing Data Sets

`mutate()` adds new variables that are functions of existing variables, and `summarise()` reduces multiple values down to a single summary.

## Mutating Data

```r
# Adding a new column that calculates yearly savings potential
mutated_data <- mutate(data, savings = salary * 0.1)
print(mutated_data)
```

## Summarizing Data

```r
# Summarizing to find the average salary
summary_data <- summarise(data, average_salary = mean(salary))
print(summary_data)
```

## Joins and Data Merging Techniques

`dplyr` provides several functions to merge data frames together, such as `left_join()`, `right_join()`, `inner_join()`, and `full_join()`.

```r
# Additional data frame with department information
department_data <- data.frame(
  name = c("Alice", "Bob", "David"),
  department = c("Finance", "IT", "Marketing")
)

# Joining data on the name column
joined_data <- left_join(data, department_data, by = "name")
print(joined_data)
```

## Group Operations with `group_by` and `summarise`

Grouping data makes it possible to compute summaries over groups of data.

```r
# Grouping data by department and calculating average salary
grouped_data <- data %>%
  left_join(department_data, by = "name") %>%
  group_by(department) %>%
  summarise(average_salary = mean(salary, na.rm = TRUE))

print(grouped_data)
```

This demonstration of `dplyr`'s capabilities shows how straightforward it is to manipulate and analyze data sets in R. These functions are not only powerful but also help make your data manipulation tasks more readable and concise.

Certainly! Let’s craft some practical exercises for this section on data handling techniques using `dplyr`. These exercises will reinforce the skills students need to manipulate and analyze data effectively with R.

## Code Exercises for Data Handling Techniques with `dplyr`

**Exercise 1: Select and Filter Data**  
Create a data frame and use `dplyr` to select specific columns and filter rows based on conditions.

```r
# Define the data frame
exercise_data <- data.frame(
  id = 1:5,
  name = c("Alice", "Bob", "Carol", "David", "Eve"),
  age = c(28, 22, 35, 19, 31),
  salary = c(62000, 52000, 58000, 48000, 59000)
)

# Use dplyr to select the name and age columns, then filter for ages greater than 25
selected_filtered_data <- exercise_data %>%
  select(name, age) %>%
  filter(age > 25)

# Print the result
print(selected_filtered_data)
```
```r
# TODO: Insert Exercise chunk with error messages and solutions.
```
**Exercise 2: Arrange and Mutate Data**  
Add a new column to the data frame that calculates the monthly salary, then arrange the data frame by this new column in descending order.

```r
# Use dplyr to add a new column for monthly salary and then arrange by it
mutated_arranged_data <- exercise_data %>%
  mutate(monthly_salary = salary / 12) %>%
  arrange(desc(monthly_salary))

# Print the result
print(mutated_arranged_data)
```
```r
# TODO: Insert Exercise chunk with error messages and solutions.
```

**Exercise 3: Summarize Data**  
Calculate the average age and median salary from the data frame.

```r
# Use dplyr to summarize the data with average age and median salary
summarized_data <- exercise_data %>%
  summarise(average_age = mean(age), median_salary = median(salary))

# Print the result
print(summarized_data)
```
```r
# TODO: Insert Exercise chunk with error messages and solutions.
```

**Exercise 4: Join Data**  
Merge two data frames on the `name` column and explore the resulting joined data.

```r
# Additional data frame with department info
department_info <- data.frame(
  name = c("Alice", "Carol", "Eve"),
  department = c("Human Resources", "Marketing", "Product Development")
)

# Use dplyr to join the data frames
joined_data <- left_join(exercise_data, department_info, by = "name")

# Print the result
print(joined_data)
```
```r
# TODO: Insert Exercise chunk with error messages and solutions.
```
**Exercise 5: Group and Summarize Data**  
Group the data by a specified column (e.g., department) and calculate the total salary for each group.

```r
# Assuming the joined_data from Exercise 4
# Group by department and calculate total salary
grouped_summarized_data <- joined_data %>%
  group_by(department) %>%
  summarise(total_salary = sum(salary, na.rm = TRUE))

# Print the result
print(grouped_summarized_data)
```
```r
# TODO: Insert Exercise chunk with error messages and solutions.
```
These exercises are designed to provide hands-on experience with real-world data manipulation tasks, reinforcing the techniques discussed in the chapter and enhancing the student’s proficiency with `dplyr`.

<!--chapter:end:02-DataStructureHandling.Rmd-->

# Data Visualization in R (Light Focus)

## Introduction to ggplot2 and Basic Plotting

Effective data visualization is key to interpreting data and communicating results clearly. In R, `ggplot2` is one of the most powerful tools for creating a wide variety of static, aesthetic, and informative plots. This section introduces `ggplot2`, its syntax, and basic plotting techniques.

### ggplot2 Syntax and Layering System

`ggplot2` is based on the grammar of graphics, a system that allows you to create graphs layer by layer. You start with data, add `aes` (aesthetics) to indicate which variables to plot, and then add `geoms` (geometric objects) to decide the type of plot.

```r
library(ggplot2)

# Basic syntax
ggplot(data = mtcars, aes(x = wt, y = mpg)) + 
  geom_point()  # adds a layer for scatter plot
```

In this example, `mtcars` is the dataset, `wt` (car weight) and `mpg` (miles per gallon) are the variables, and `geom_point()` creates a scatter plot.

### Creating Histograms, Bar Plots, and Scatter Plots

Histograms, bar plots, and scatter plots are fundamental for exploring data distributions and relationships between variables.

#### Histograms

Histograms are used to visualize the distribution of a single continuous variable by dividing the data into bins and counting the number of observations in each bin.

```r
# Creating a histogram of car weights
ggplot(mtcars, aes(x = wt)) + 
  geom_histogram(binwidth = 0.5, fill = "blue", color = "black")
```

#### Bar Plots

Bar plots are useful for comparing quantities corresponding to different groups.

```r
# Convert cyl (number of cylinders) to a factor for a bar plot
mtcars$cyl <- as.factor(mtcars$cyl)

# Creating a bar plot of car counts per cylinder type
ggplot(mtcars, aes(x = cyl)) +
  geom_bar(fill = "tomato", color = "black")
```

#### Scatter Plots

Scatter plots are ideal for examining the relationship between two continuous variables.

```r
# Creating a scatter plot of mpg vs wt
ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point(size = 3, color = "dodgerblue")
```

### Aesthetics and Themes

`ggplot2` allows extensive customization of plots through aesthetics and themes, enabling you to make plots more informative and appealing.

```r
# Enhancing scatter plot with themes and labels
ggplot(mtcars, aes(x = wt, y = mpg, color = cyl)) +  # Color points by cylinder count
  geom_point(size = 4) +
  labs(title = "Car Weight vs. MPG",
       x = "Weight (1000 lbs)",
       y = "Miles per Gallon",
       color = "Number of Cylinders") +
  theme_minimal()  # Using a minimal theme
```

These examples demonstrate basic `ggplot2` usage for creating various types of plots, setting the stage for more advanced visualization techniques. Each type of plot serves different purposes and can be tailored extensively using `ggplot2`'s powerful customization options.


## Code Exercises 1

**Exercise 1: Create a Histogram**
Task: Use the `ggplot2` package to create a histogram of the `hp` (horsepower) variable from the `mtcars` dataset. Customize the bin width to 20 and set the fill color to green.

```r
library(ggplot2)

# Task: Create a histogram of the hp variable from the mtcars dataset
# Customize the bin width and fill color
ggplot(mtcars, aes(x = hp)) +
  geom_histogram(binwidth = 20, fill = "green") +
  labs(title = "Histogram of Car Horsepower",
       x = "Horsepower",
       y = "Frequency")
```
```r
# TODO: Insert Exercise chunk with error messages and solutions.
```

**Exercise 2: Create a Bar Plot**
Task: Create a bar plot showing the number of cars with each number of gears (`gear`) in the `mtcars` dataset. Use the color red for the bars.

```r
# Task: Create a bar plot of the gear variable from the mtcars dataset
ggplot(mtcars, aes(x = factor(gear))) +
  geom_bar(fill = "red") +
  labs(title = "Number of Cars by Gear Count",
       x = "Number of Gears",
       y = "Count")
```
```r
# TODO: Insert Exercise chunk with error messages and solutions.
```

**Exercise 3: Create a Scatter Plot**
Task: Plot `mpg` (miles per gallon) against `disp` (displacement) using `ggplot2`. Color the points by `cyl` (number of cylinders), and add a smooth line to the plot.

```r
# Task: Create a scatter plot of mpg vs disp, color by cyl, and add a smooth line
ggplot(mtcars, aes(x = disp, y = mpg, color = factor(cyl))) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "MPG vs. Displacement",
       x = "Displacement",
       y = "Miles per Gallon",
       color = "Number of Cylinders")
```
```r
# TODO: Insert Exercise chunk with error messages and solutions.
```
**Exercise 4: Customize a Scatter Plot with Themes**
Task: Enhance the previous scatter plot by applying the `theme_bw()` theme and customizing the plot's text elements and legends.

```r
# Task: Enhance the scatter plot with theme_bw and customize text and legends
ggplot(mtcars, aes(x = disp, y = mpg, color = factor(cyl))) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "MPG vs. Displacement: Enhanced Visualization",
       x = "Displacement (cc)",
       y = "Miles per Gallon",
       color = "Cylinder Count") +
  theme_bw() +
  theme(
    plot.title = element_text(hjust = 0.5),
    axis.title = element_text(face = "bold"),
    legend.position = "top"
  )
```
```r
# TODO: Insert Exercise chunk with error messages and solutions.
```

These exercises encourage students to apply their knowledge of ggplot2 for creating, customizing, and enhancing visual representations of data. By completing these tasks, students will gain hands-on experience in effectively communicating data insights through visualizations.


# Customizing Graphs for Data Presentation

Visualizing data effectively involves more than just creating basic plots; it requires customization to enhance clarity and impact. This section of the course will teach you how to customize axes, legends, and labels, fine-tune scales and coordinates, and add annotations and custom geometries using ggplot2 in R.

## Customizing Axes, Legends, and Labels

Customizing the textual components of your plots helps in making them more readable and informative. `ggplot2` allows for detailed adjustments to these elements.

### Customizing Axes

You can modify the text of axis labels and adjust the axis limits to focus on specific areas of the data.

```r
library(ggplot2)

# Example of customizing axes
ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point() +
  scale_x_continuous("Weight (in 1000 lbs)", limits = c(1, 6)) +
  scale_y_continuous("Miles Per Gallon", limits = c(10, 35)) +
  labs(title = "Car Weight vs. Fuel Efficiency")
```

### Customizing Legends

Adjusting the legend involves changing its title, text, and position to improve the overall aesthetics and readability.

```r
# Example of customizing legends
ggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +
  geom_point() +
  labs(color = "Number of Cylinders") +
  theme(legend.position = "bottom")
```

### Customizing Labels

Labels for the entire plot, axes, and legends can be styled to enhance clarity or to align with publication standards or personal preferences.

```r
# Example of customizing labels
ggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +
  geom_point() +
  labs(
    title = "Vehicle Dynamics",
    x = "Car Weight (1000 lbs)",
    y = "Fuel Efficiency (MPG)",
    color = "Cylinders"
  )
```

## Fine-tuning Scales and Coordinates

Manipulating scales and coordinates can help in presenting data more effectively by adjusting the visual representation to better fit the dataset's story.

### Scales

Scales adjust how data is mapped to aesthetics. You can set the scale for each aesthetic (like color, size, or shape) to control the visual properties of the plot.

```r
# Example of adjusting color scales
ggplot(mtcars, aes(x = wt, y = mpg, color = hp)) +
  geom_point() +
  scale_color_gradient(low = "blue", high = "red")
```

### Coordinates

Using coordinate functions, you can adjust the plotting area, aspect ratios, or flip axes which is useful for certain types of data or specific visual effects.

```r
# Example of flipping coordinates
ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point() +
  coord_flip()
```

## Adding Annotations and Custom Geometries

Annotations and custom geometries enhance plots by adding text annotations, lines, or other shapes to highlight specific features or data points.

### Annotations

Adding text annotations to highlight specific points or areas on the plot.

```r
# Example of adding annotations
ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point() +
  annotate("text", x = 5, y = 30, label = "Outlier", color = "red")
```

### Custom Geometries

You can draw shapes or lines to highlight trends or important areas.

```r
# Example of adding custom geometries
ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point() +
  geom_hline(yintercept = 20, linetype = "dashed", color = "gray")
```

These customization techniques in ggplot2 enable you to tailor your visualizations precisely to your data’s narrative, improving not only the aesthetics but also the effectiveness of your data presentations.

## Code Exercises 2

**Exercise 1: Customize Axis Labels and Limits**
Task: Create a scatter plot of `disp` (displacement) vs `mpg` (miles per gallon) from the `mtcars` dataset. Customize the axis labels to be more descriptive and set appropriate limits for better data focus.

```r
library(ggplot2)

# Task: Customize axis labels and limits
ggplot(mtcars, aes(x = disp, y = mpg)) +
  geom_point() +
  scale_x_continuous("Engine Displacement (cc)", limits = c(100, 500)) +
  scale_y_continuous("Miles per Gallon", limits = c(10, 35)) +
  labs(title = "Engine Displacement vs. Fuel Efficiency")
```

**Exercise 2: Customize and Reposition the Legend**
Task: Plot `wt` (weight) against `mpg` with `cyl` (number of cylinders) as a color factor. Customize the legend title and reposition it to the top of the plot.

```r
# Task: Customize legend title and reposition
ggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +
  geom_point() +
  labs(color = "Cylinder Count") +
  theme(legend.position = "top") +
  labs(title = "Car Weight vs. Fuel Efficiency by Cylinder Count")
```

**Exercise 3: Add Annotations to Highlight Specific Data Points**
Task: Highlight vehicles with mpg greater than 30 or disp less than 120. Add a custom text annotation near these points.

```r
# Task: Highlight specific data points with annotations
ggplot(mtcars, aes(x = disp, y = mpg)) +
  geom_point() +
  geom_text(aes(label = ifelse(mpg > 30 | disp < 120, as.character(mpg), "")),
            hjust = -0.1, vjust = 0) +
  labs(title = "Highlighting Efficient Vehicles")
```

**Exercise 4: Add Custom Geometries to a Plot**
Task: Create a plot of `hp` (horsepower) against `mpg`. Add a horizontal dashed line at the median horsepower and a vertical solid line at the median mpg.

```r
# Task: Add custom lines to indicate median values
median_hp <- median(mtcars$hp)
median_mpg <- median(mtcars$mpg)

ggplot(mtcars, aes(x = mpg, y = hp)) +
  geom_point() +
  geom_hline(yintercept = median_hp, linetype = "dashed", color = "red") +
  geom_vline(xintercept = median_mpg, linetype = "solid", color = "blue") +
  labs(title = "Horsepower vs. Fuel Efficiency with Median Lines")
```


<!--chapter:end:03-DataVisualization.Rmd-->

# Working with Different Types of Data (Light Focus)

## Overview of Qualitative and Quantitative Data

Understanding the distinction between qualitative and quantitative data is fundamental in data analysis, as it informs the appropriate methods for processing and analyzing data. This section provides a detailed explanation and demonstrations of these two major types of data.

### Defining Qualitative vs Quantitative Data

**Qualitative Data**  
Qualitative data, also known as categorical data, represents attributes, labels, or non-numeric entries. It describes qualities or characteristics that are observed and recorded. This type of data is often textual but may also include images or objects.

- **Examples**: Colors (red, blue, green), preferences (like, dislike), product types (household, industrial), or any labels categorizing data such as names of places or people.

**Demonstration of Qualitative Data**:
```r
# Sample qualitative data: Survey responses
survey_data <- data.frame(
  respondent_id = 1:3,
  favorite_color = c("Blue", "Red", "Green"),
  satisfaction_level = c("High", "Medium", "Low")  # This could be converted to quantitative data for analysis.
)

print(survey_data)
```

**Quantitative Data**  
Quantitative data involves numerical measurements or counts that can be quantified. This data type can be further classified as either discrete or continuous.

- **Discrete data**: Numeric data that has a countable number of values between any two values. A discrete variable cannot take the value of a fraction between one value and the next closest value.
  - **Example**: The number of users subscribed to a service, the number of cars in a parking lot.

- **Continuous data**: Numeric data that can take any value within a finite or infinite interval. It can represent measurements and therefore can take any value in an interval.
  - **Example**: Temperature, weight, height, and time.

**Demonstration of Quantitative Data**:
```r
# Sample quantitative data: Company sales data
sales_data <- data.frame(
  month = c("January", "February", "March"),
  revenue = c(10000, 15000, 13000),  # Continuous data
  units_sold = c(10, 15, 13)         # Discrete data
)

print(sales_data)
```

### Sampling Strategies and Data Collection Methods

When working with either qualitative or quantitative data, choosing the right sampling strategy and data collection method is crucial for obtaining reliable and representative data.

- **Random Sampling**: Ensures every member of the population has an equal chance of being selected. Reduces sampling bias.
- **Stratified Sampling**: Divides the population into smaller groups, or strata, based on shared characteristics before sampling. This is useful when the population is heterogeneous.

**Demonstration of Sampling Strategies**:
```r
# Assuming a larger dataset with multiple categories
data <- data.frame(
  category = rep(c("A", "B", "C"), times = 100),
  value = rnorm(300)
)

# Stratified sampling: Sample 10 from each category
sampled_data <- do.call(rbind, lapply(split(data, data$category), function(subdata) {
  subdata[sample(nrow(subdata), 10), ]
}))

print(sampled_data)
```

### Structuring Data Sets for Analysis

Properly structuring data sets is essential for analysis, particularly when dealing with mixed data types. Structured data ensures consistency and ease of access during analysis.

- **Data Frames**: R's primary data structure for storing datasets with multiple types of information.
- **Matrix**: Suitable for numerical data where operations like matrix multiplication are needed.

**Demonstration of Data Structuring**:
```r
# Creating a structured data frame
structured_data <- data.frame(
  ID = 1:5,
  Name = c("Alice", "Bob", "Carol", "David", "Eve"),
  Age = c(25, 30, 35, 40, 45),
  Salary = c(50000, 60000, 55000, 58000, 62000)
)

print(structured_data)
```

This section has introduced the foundational concepts of qualitative and quantitative data, illustrated with R examples. Understanding these distinctions and how to work with different data types prepares you for more effective data analysis and interpretation.


## Code Exercises 1

**Exercise 1: Identify Data Types**
Task: Given a data frame, identify which columns contain qualitative data and which contain quantitative data. Provide a brief explanation for each identification.

```r
# Sample data frame for identification
data <- data.frame(
  employee_id = 1:4,
  department = c("Marketing", "Finance", "HR", "IT"),
  years_of_experience = c(5, 3, 8, 2),
  gender = c("Female", "Male", "Female", "Male")
)

# Task: Identify the type of data in each column and print your results
# Print commands for students to fill out
print("employee_id: Quantitative - Discrete (counts the number of employees)")
print("department: Qualitative - Categorical (labels representing departments)")
print("years_of_experience: Quantitative - Discrete (counts years)")
print("gender: Qualitative - Categorical (labels representing genders)")
```
```r
# TODO: Insert Exercise chunk with error messages and solutions.
```
**Exercise 2: Implement Random Sampling**
Task: From a given vector of 100 integers, perform a simple random sampling to select 10 integers.

```r
# Generating a vector of 100 integers
integers <- 1:100

# Task: Perform random sampling to select 10 integers
sampled_integers <- sample(integers, 10)

# Print the sampled integers
print(sampled_integers)
```
```r
# TODO: Insert Exercise chunk with error messages and solutions.
```
**Exercise 3: Create a Stratified Sample**
Given a data frame with student data including their major and GPA, create a stratified sample where you sample 5 students from each major.

```r
# Sample student data
students <- data.frame(
  id = 1:50,
  major = rep(c("Biology", "Chemistry", "Physics"), length.out = 50),
  gpa = runif(50, 2.0, 4.0)
)

# Task: Create a stratified sample where you sample 5 students from each major
library(dplyr)
stratified_sample <- students %>%
  group_by(major) %>%
  sample_n(5)

# Print the stratified sample
print(stratified_sample)
```
```r
# TODO: Insert Exercise chunk with error messages and solutions.
```

**Exercise 4: Structuring Data Sets for Analysis**
Task: Convert the provided list of vectors into a data frame, then add a new column that categorizes the 'age' into 'Youth', 'Adult', or 'Senior'.

```r
# Provided list of vectors
person_list <- list(
  names = c("Alice", "Bob", "Carol"),
  ages = c(22, 35, 77),
  salaries = c(45000, 55000, 67000)
)

# Task: Convert list to data frame and categorize age
person_df <- data.frame(person_list)
person_df$category <- ifelse(person_df$ages < 30, "Youth",
                             ifelse(person_df$ages < 60, "Adult", "Senior"))

# Print the resulting data frame
print(person_df)
```
```r
# TODO: Insert Exercise chunk with error messages and solutions.
```

# Best Practices in Data Collection and Cleaning

Effective data collection and cleaning are crucial for ensuring the accuracy and reliability of data analysis. This section covers essential practices for identifying and handling missing data, performing data type conversions and formatting, and detecting and dealing with outliers.

## Identifying and Handling Missing Data

Missing data can significantly impact the results of your analysis. Properly identifying and handling missing entries is a critical step in data cleaning.

### Identifying Missing Data

You can identify missing data in R using functions like `is.na()` which returns a logical vector indicating which elements are NA.

```r
# Example dataset with missing values
data <- data.frame(
  names = c("Alice", "Bob", NA, "David"),
  scores = c(92, NA, 88, 94)
)

# Identifying missing data
missing_data <- is.na(data)
print(missing_data)
```

### Handling Missing Data

Once identified, you can handle missing data by either removing it or imputing it based on the context of your analysis.

- **Removing missing data**: Use `na.omit()` to remove rows with NA values.
- **Imputing missing data**: Replace missing values with statistical measures (mean, median) or more sophisticated algorithms.

```r
# Removing missing data
clean_data <- na.omit(data)
print(clean_data)

# Imputing missing data with the mean
data$scores[is.na(data$scores)] <- mean(data$scores, na.rm = TRUE)
print(data)
```

## Data Type Conversions and Formatting

Data often comes in formats that are not suitable for analysis. Converting data into the correct type is essential for further processing.

### Data Type Conversion

Use functions like `as.numeric()`, `as.factor()`, etc., to convert data types in R.

```r
# Example of converting character data to numeric
data$ages <- c("20", "25", "30", "35")
data$ages <- as.numeric(data$ages)
print(data$ages)
```

### Formatting Data

Formatting data for readability and consistency (e.g., date formats) ensures that your datasets are easy to understand and analyze.

```r
# Formatting dates
data$dates <- c("01-01-2020", "02-01-2020", "03-01-2020")
data$dates <- as.Date(data$dates, format = "%d-%m-%Y")
print(data$dates)
```

## Detecting and Dealing with Outliers

Outliers can skew the results of your data analysis, making it important to detect and appropriately handle them.

### Detecting Outliers

A common method to detect outliers is to use statistical thresholds like the interquartile range (IQR).

```r
# Detecting outliers using IQR
scores <- c(100, 102, 99, 105, 110, 200, 98, 97, 95, 250)
Q1 <- quantile(scores, 0.25)
Q3 <- quantile(scores, 0.75)
IQR <- Q3 - Q1
outliers <- scores < (Q1 - 1.5 * IQR) | scores > (Q3 + 1.5 * IQR)
print(scores[outliers])
```

### Handling Outliers

Handling outliers depends on their cause and the amount of impact they have on your analysis.

- **Removing outliers**: If they are errors or extremely rare.
- **Capping values**: Replacing outliers with the highest non-outlier values.

```r
# Removing outliers
clean_scores <- scores[!outliers]
print(clean_scores)

# Capping outliers
scores[scores < (Q1 - 1.5 * IQR)] <- Q1 - 1.5 * IQR
scores[scores > (Q3 + 1.5 * IQR)] <- Q3 + 1.5 * IQR
print(scores)
```



## Code Exercises 2

**Exercise 1: Identify and Handle Missing Data**
Task: Given a data frame with missing values, identify the missing values and then fill these missing values with the column mean.

```r
# Provided data frame
data <- data.frame(
  name = c("Alice", "Bob", NA, "David"),
  score = c(85, NA, 88, 90)
)

# Task: Identify missing values
print("Missing Data Identification:")
print(is.na(data))

# Task: Fill missing scores with the column mean
data$score[is.na(data$score)] <- mean(data$score, na.rm = TRUE)
print("Data after handling missing values:")
print(data)
```
```r
# TODO: Insert Exercise chunk with error messages and solutions.
```

**Exercise 2: Convert Data Types**
Task: Convert character data representing integers into numeric data, then verify the conversion by checking the data type.

```r
# Character data
char_data <- data.frame(
  values = c("100", "200", "300", "400")
)

# Task: Convert character to numeric
char_data$values <- as.numeric(char_data$values)

# Task: Print the converted data and data type
print("Converted Data:")
print(char_data)
print("Data Type Verification:")
print(sapply(char_data, class))
```
```r
# TODO: Insert Exercise chunk with error messages and solutions.
```

**Exercise 3: Detect and Handle Outliers**
Task: Given a vector of data, detect outliers using the IQR method and replace them with the median of the dataset.

```r
# Provided data
values <- c(50, 55, 45, 60, 400, 65, 50, 430, 49, 52)

# Task: Detect outliers
Q1 <- quantile(values, 0.25)
Q3 <- quantile(values, 0.75)
IQR <- Q3 - Q1
outliers <- which(values < (Q1 - 1.5 * IQR) | values > (Q3 + 1.5 * IQR))

# Task: Replace outliers with the median
values[outliers] <- median(values)

# Print the cleaned data
print("Cleaned Data:")
print(values)
```
```r
# TODO: Insert Exercise chunk with error messages and solutions.
```

**Exercise 4: Date Formatting**
Task: Convert a vector of date strings from the format "dd-mm-yyyy" to R Date objects and then change their format to "yyyy/mm/dd".

```r
# Provided date strings
date_strings <- c("01-12-2022", "15-11-2022", "23-10-2022")

# Task: Convert to Date object
dates <- as.Date(date_strings, format = "%d-%m-%Y")

# Task: Reformat dates to "yyyy/mm/dd"
formatted_dates <- format(dates, "%Y/%m/%d")

# Print the formatted dates
print("Formatted Dates:")
print(formatted_dates)
```
```r
# TODO: Insert Exercise chunk with error messages and solutions.
```

<!--chapter:end:04-DataAnalysisTypes.Rmd-->

# Descriptive Statistics

## Measures of Central Tendency, Variability, and Standard Scores

Understanding the central tendency and variability of data is crucial in descriptive statistics. These measures give us insights into the general pattern of the data, its central position, and its spread. Additionally, standard scores allow us to understand how individual data points relate to the distribution. Here's a breakdown of how to calculate these statistics and their importance.

### Calculating Mean, Median, and Mode

The mean, median, and mode are measures of central tendency that describe the center of a data set.

- **Mean** is the average value and is affected by outliers.
- **Median** is the middle value in a data set when ordered from the smallest to the largest and is less affected by outliers.
- **Mode** is the most frequently occurring value in a data set and can be used for both numerical and categorical data.

**Example in R:**
```r
# Sample data
data <- c(2, 3, 3, 5, 7, 10, 11)

# Calculating mean
mean_value <- mean(data)
print(paste("Mean:", mean_value))

# Calculating median
median_value <- median(data)
print(paste("Median:", median_value))

# Calculating mode
get_mode <- function(x) {
  uniqx <- unique(x)
  uniqx[which.max(tabulate(match(x, uniqx)))]
}
mode_value <- get_mode(data)
print(paste("Mode:", mode_value))
```

### Variance, Standard Deviation, and Range

These are measures of variability that indicate the spread of data points.

- **Variance** measures the average squared deviation of each number from the mean. It gives a sense of how data is spread out.
- **Standard Deviation** is the square root of variance and provides a metric that is in the same units as the data.
- **Range** is the difference between the maximum and minimum values in the dataset.

**Example in R:**
```r
# Calculating variance
variance_value <- var(data)
print(paste("Variance:", variance_value))

# Calculating standard deviation
sd_value <- sd(data)
print(paste("Standard Deviation:", sd_value))

# Calculating range
range_value <- max(data) - min(data)
print(paste("Range:", range_value))
```

### Normalization and Standardization of Scores

Normalization and standardization are techniques to adjust the scale of data, which is crucial for many statistical techniques and data visualization.

- **Normalization** typically refers to the process of scaling data to have a minimum of 0 and a maximum of 1.
- **Standardization** (Z-score normalization) involves rescaling data to have a mean of 0 and a standard deviation of 1.

**Example in R:**
```r
# Normalization
normalize <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}
normalized_data <- normalize(data)
print("Normalized Data:")
print(normalized_data)

# Standardization
standardize <- function(x) {
  (x - mean(x)) / sd(x)
}
standardized_data <- standardize(data)
print("Standardized Data:")
print(standardized_data)
```

### Graphical Representations

Visual tools like histograms and box plots are invaluable for understanding data distributions.

**Histogram Example:**
```r
library(ggplot2)
ggplot(data.frame(data), aes(x=data)) + 
  geom_histogram(binwidth = 1, fill="blue") +
  ggtitle("Histogram of Data")
```

**Box Plot Example:**
```r
ggplot(data.frame(data), aes(y=data)) + 
  geom_boxplot(fill="tomato") +
  ggtitle("Box Plot of Data")
```

### Real-World Applications

In business, statistics guide decision-making processes; in science, they validate research findings.


## Code Exercises 1

**Exercise 1: Compute and Visualize Descriptive Statistics**  
Task: Given a dataset of daily temperatures, calculate the mean, median, mode, variance, and standard deviation. Additionally, create a histogram and a box plot to visualize the data distribution.

```r
# Sample data: Daily temperatures
temperatures <- c(22, 24, 24, 18, 30, 32, 19, 21, 24, 20, 23, 19, 22, 25)

# Task: Calculate descriptive statistics
mean_temp <- mean(temperatures)
median_temp <- median(temperatures)
mode_temp <- get_mode(temperatures)  # Assuming get_mode is a pre-defined function
variance_temp <- var(temperatures)
sd_temp <- sd(temperatures)

# Task: Print the calculated statistics
print(paste("Mean Temperature:", mean_temp))
print(paste("Median Temperature:", median_temp))
print(paste("Mode Temperature:", mode_temp))
print(paste("Variance of Temperature:", variance_temp))
print(paste("Standard Deviation of Temperature:", sd_temp))

# Task: Visualize the data
library(ggplot2)
ggplot(data.frame(temperatures), aes(x=temperatures)) +
  geom_histogram(binwidth = 1, fill='blue', color='black') +
  ggtitle("Histogram of Daily Temperatures")

ggplot(data.frame(temperatures), aes(y=temperatures)) +
  geom_boxplot(fill='tomato') +
  ggtitle("Box Plot of Daily Temperatures")
```
```r
# TODO: Insert Exercise chunk with error messages and solutions.
```

**Exercise 2: Normalize and Standardize a Dataset**  
Task: Normalize and standardize the given sales data. Discuss how these transformations might affect further data analysis.

```r
# Sample sales data
sales <- c(200, 300, 400, 600, 600, 700, 100)

# Task: Normalize and standardize the sales data
normalized_sales <- (sales - min(sales)) / (max(sales) - min(sales))
standardized_sales <- (sales - mean(sales)) / sd(sales)

# Task: Print the transformed data
print("Normalized Sales:")
print(normalized_sales)
print("Standardized Sales:")
print(standardized_sales)
```
```r
# TODO: Insert Exercise chunk with error messages and solutions.
```

**Exercise 3: Real-World Application Scenario**  
Imagine you're a data analyst at a retail company. You have the following monthly sales data for different products. Calculate the average sales, identify any outliers, and determine which products are underperforming.

```r
# Monthly sales data for products
monthly_sales <- c(500, 600, 150, 200, 650, 700, 580, 250, 300, 450, 420, 500)

# Task: Calculate average sales and detect outliers
average_sales <- mean(monthly_sales)
sales_sd <- sd(monthly_sales)
outliers <- monthly_sales[monthly_sales > (average_sales + 2 * sales_sd) | monthly_sales < (average_sales - 2 * sales_sd)]

# Task: Identify underperforming products (below average)
underperforming <- monthly_sales[monthly_sales < average_sales]

# Task: Print results
print(paste("Average Sales:", average_sales))
print("Outliers in Sales:")
print(outliers)
print("Underperforming Product Sales:")
print(underperforming)
```
```r
# TODO: Insert Exercise chunk with error messages and solutions.
```

# More on Visualizing Distributions and Relationships

Enhancing data visualization skills is crucial for analyzing and interpreting complex datasets. This section covers advanced visualization techniques like box plots, violin plots, density plots, scatter plots, correlograms, and introduces multidimensional scaling (MDS) and principal component analysis (PCA).

## Box Plots, Violin Plots, and Density Plots

These plots are essential for visualizing the distribution of data and identifying outliers.

### Box Plots
Box plots provide a graphical representation of the central tendency and variability of data, highlighting medians and quartiles.

```r
library(ggplot2)

# Example of a Box Plot
data <- mtcars$mpg
ggplot(mtcars, aes(x = factor(0), y = data)) +
  geom_boxplot(fill = "lightblue") +
  labs(title = "Box Plot of Miles Per Gallon", x = "", y = "Miles Per Gallon")
```

### Violin Plots
Violin plots combine box plots and density plots, providing a deeper understanding of the density distribution.

```r
# Example of a Violin Plot
ggplot(mtcars, aes(x = factor(cyl), y = mpg)) +
  geom_violin(fill = "lightgreen") +
  labs(title = "Violin Plot of Miles Per Gallon by Cylinder Count", x = "Number of Cylinders", y = "Miles Per Gallon")
```

### Density Plots
Density plots are smoothed, continuous versions of histograms that estimate the probability density function of a variable.

```r
# Example of a Density Plot
ggplot(mtcars, aes(x = mpg)) +
  geom_density(fill = "salmon") +
  labs(title = "Density Plot of Miles Per Gallon", x = "Miles Per Gallon", y = "Density")
```

## Pairwise Relationships with Scatter Plots and Correlograms

Understanding the relationships between multiple variables is crucial for many analytical tasks.

### Scatter Plots
Scatter plots allow for the visualization of relationships between two numeric variables.

```r
# Example of a Scatter Plot
ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point(color = "blue") +
  labs(title = "Scatter Plot of Weight vs Miles Per Gallon", x = "Weight (1000 lbs)", y = "Miles Per Gallon")
```

### Correlograms
Correlograms show the correlation coefficients between pairs of variables across an entire dataset, useful for feature selection.

```r
# Correlogram with the PerformanceAnalytics package
library(PerformanceAnalytics)
chart.Correlation(mtcars[,1:7], histogram = TRUE, pch = 19)
```

## Introduction to Multidimensional Scaling and PCA

These techniques are used for dimensionality reduction, allowing for the visualization of complex, high-dimensional data in a lower-dimensional space.

### Multidimensional Scaling (MDS)
MDS projects high-dimensional data into a lower-dimensional space while preserving the distances among data points.

```r
# Example of MDS
distances <- dist(mtcars)  # Calculate Euclidean distances
mds_model <- cmdscale(distances)  # Classical MDS
plot(mds_model[,1], mds_model[,2], xlab = "MDS1", ylab = "MDS2", main = "MDS Plot of mtcars")
```

### Principal Component Analysis (PCA)
PCA reduces dimensionality by transforming variables into a new set of variables (principal components), which are linear combinations of the original variables.

```r
# Example of PCA
pca_result <- prcomp(mtcars[, c("mpg", "disp", "hp", "drat")], scale. = TRUE)
plot(pca_result$x[,1:2], xlab = "PC1", ylab = "PC2", main = "PCA of Selected mtcars Variables")
```

## Exercise for Visualizing Distributions and Relationships

**Exercise: Apply Visualization Techniques**
Given the `iris` dataset, create box plots for each species' petal length, a correlogram for the dataset, and perform PCA to visualize the first two principal components.

```r
# Load data
data(iris)

# Box plots for each species' petal length
ggplot(iris, aes(x = Species, y = Petal.Length)) +
  geom_boxplot() +
  labs(title = "Box Plot of Petal Length by Species", x = "Species", y = "Petal Length")

# Correlogram for iris dataset
pairs(~Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data = iris, col = iris$Species)

# PCA on the iris dataset
iris_pca <- prcomp(iris[,1

:4], scale. = TRUE)
plot(iris_pca$x[,1:2], col = iris$Species, xlab = "PC1", ylab = "PC2", main = "PCA of Iris Dataset")
```
```r
# TODO: Insert Exercise chunk with error messages and solutions.
```

<!--chapter:end:05-DescriptiveStatistics.Rmd-->

# Probability Distributions and Statistical Concepts

## Overview
This section delves into the fundamental concepts and applications of probability distributions, key statistical theorems, and methods for analyzing random variables. We aim to build a foundational understanding, enriched by practical examples and real-world applications.

## Understanding and Applying Common Probability Distributions

### Objective
Explore different types of probability distributions and their applications to effectively model and analyze data in various scenarios.

1. **Binomial, Poisson, and Normal Distributions**
   - The **Binomial Distribution** is applicable when modeling the number of successes in a fixed number of independent trials. For example, predicting the number of heads in 10 coin tosses or determining the probability of a certain number of defective items in a batch.
   
   - The **Poisson Distribution** is ideal for modeling the number of events in a fixed interval of time or space when these events happen with a known constant rate. It's used for things like counting the number of calls at a call center over an hour or the arrival of buses at a station.
   
   - The **Normal Distribution** is a crucial tool in statistics, representing data that clusters around a mean or average. It is the basis for many statistical tests and is used to model everything from heights and test scores to measurement errors.

   **Let's look at these distributions in action with R:**
   ```r
   # Binomial example: Probability of exactly 6 heads in 10 fair coin tosses
   print(dbinom(6, size=10, prob=0.5))
   
   # Poisson example: Probability of receiving 3 emails in an hour if the average rate is 5 emails per hour
   print(dpois(3, lambda=5))
   
   # Normal distribution example: Probability density of a score at the mean of a distribution
   print(dnorm(0, mean=0, sd=1))
   ```

2. **Continuous vs Discrete Distributions**
   - **Continuous distributions** are used for data that can take any value within an interval. Measurements like weight, temperature, and distance, where data can vary continuously, are modeled using continuous distributions.
   
   - **Discrete distributions** are used for countable data. Whenever you count something, like the number of students in a class or the number of cars in a parking lot, you are working with discrete data.

   **Here's how you might visualize these differences with R:**
   ```r
   # Continuous distribution visualization
   plot(density(rnorm(1000)), main="Continuous Distribution Example: Normal Distribution")
   
   # Discrete distribution visualization
   barplot(dbinom(0:10, size=10, prob=0.5), names.arg=0:10, main="Discrete Distribution Example: Binomial Distribution")
   ```

3. **The Central Limit Theorem (CLT)**
   - A cornerstone of statistics, the CLT states that the distribution of sample means approximates a normal distribution as the sample size becomes large, regardless of the population distribution's shape. This theorem underpins many statistical methods, including hypothesis testing and the creation of confidence intervals.

   **Experience the CLT through a simple R simulation:**
   ```r
   # Simulating sample means
   sample_means <- replicate(1000, mean(runif(50, min=0, max=1)))
   hist(sample_means, probability=TRUE, main="CLT Simulation")
   lines(density(sample_means), col="red")
   ```


# Exploring Random Variables and Expected Values

## Objective
The goal of this section is to delve into the statistical fundamentals of random variables and expected values, elaborating on their theoretical underpinnings, methods of calculation, and practical implications in data analysis.

## Subsections

1. **Defining and Calculating Expected Values**
   - **Definition**: The expected value of a random variable gives a measure of the center of the distribution of the variable. Essentially, it is the long-term average value of the variable when the experiment it represents is repeated infinitely many times.
   - **Formula**: For a discrete random variable X with possible values x₁, x₂, ..., xₙ and probabilities p₁, p₂, ..., pₙ, the expected value E(X) is calculated as:
     \[
     E(X) = \sum_{i=1}^{n} x_i p_i
     \]
   - **Calculation Example**: Consider a simple dice roll where each face is equally likely. The expected value of the dice roll can be computed as follows:
     ```r
     values <- 1:6
     probabilities <- rep(1/6, 6)
     expected_value <- sum(values * probabilities)
     print(paste("Expected Value of a Dice Roll:", expected_value))
     ```

2. **Variance and Standard Deviation of Random Variables**
   - **Variance**: Measures the spread of the random variable from its expected value, quantifying the average squared deviations from the mean. It is represented mathematically as:
     \[
     \text{Var}(X) = E[(X - \mu)^2]
     \]
   - **Standard Deviation**: The square root of the variance, providing a measure of spread that is in the same units as the data.
   - **Demonstration**:
     ```r
     # Continuing with the dice example
     mean_dice <- mean(values)
     variance_dice <- sum((values - mean_dice)^2 * probabilities)
     sd_dice <- sqrt(variance_dice)
     print(paste("Variance of Dice Roll:", variance_dice))
     print(paste("Standard Deviation of Dice Roll:", sd_dice))
     ```

3. **The Law of Large Numbers**
   - **Explanation**: This fundamental theorem states that as the number of trials in a random experiment increases, the average of the results obtained should get closer to the expected value. It forms the basis for frequency-based probability.
   - **Practical Implications**: Demonstrates that empirical probabilities converge to theoretical probabilities as the number of trials increases, which is critical for simulations and practical estimations in statistics.
   - **Demonstration**:
     ```r
     # Simulating repeated dice rolls
     set.seed(123)  # For reproducibility
     rolls <- replicate(10000, mean(sample(values, 100, replace=TRUE)))
     hist(rolls, breaks=30, main="Distribution of Average Dice Rolls Over 10,000 Simulations",
          xlab="Average Roll", ylab="Frequency", col="lightblue")
     abline(v=expected_value, col="red", lwd=2)
     ```


# Practical Applications and Demonstrations of Statistical Concepts

## Introduction

This section is designed to transition from theoretical understanding to practical application, allowing students to apply statistical concepts to real-world data. Through hands-on activities, we aim to deepen comprehension and enhance analytical skills, preparing students for real-world challenges.

## Engaging with Simulations: Understanding Through Action

### Simulating Statistical Distributions
- **Objective**: Experience firsthand the properties and behaviors of different statistical distributions.
- **Activity**:
  - Generate data samples from binomial, Poisson, and normal distributions.
  - Analyze the shape and spread of these distributions under various parameters to observe theoretical properties in practice.

  **Example: Simulating a Normal Distribution**
  ```r
  # Generate a sample from a normal distribution
  sample_normal <- rnorm(1000, mean = 50, sd = 10)
  
  # Visualize the distribution
  library(ggplot2)
  ggplot(data.frame(Value = sample_normal), aes(x = Value)) +
    geom_histogram(bins = 30, fill = 'steelblue', color = 'black') +
    labs(title = "Visualization of a Normal Distribution", x = "Values", y = "Frequency")
  ```

### Visual Insights: Plotting Distributions
- **Objective**: Cultivate the ability to visually interpret statistical data through plotting.
- **Activity**:
  - Use R to create dynamic visualizations (histograms, density plots) that illustrate the distribution of data.
  - Discuss how visualization aids in understanding data distributions and identifying outliers or patterns.

## Analyzing Data: From Theory to Practice

### Data-Driven Decision Making
- **Objective**: Apply statistical analysis to derive meaningful insights and make informed decisions.
- **Activity**:
  - Conduct a detailed analysis of a dataset by calculating descriptive statistics, applying probability distributions, and testing hypotheses.
  - Interpret the results to make decisions or predictions based on data.

  **Example: Sales Data Analysis**
  ```r
  # Dataset preparation
  sales_data <- data.frame(
      Region = rep(c("North", "South", "East", "West"), each = 250),
      Sales = c(rnorm(250, mean = 200, sd = 50), rnorm(250, mean = 150, sd = 30),
                rnorm(250, mean = 300, sd = 70), rnorm(250, mean = 250, sd = 40))
  )
  
  # Analysis with ggplot2
  ggplot(sales_data, aes(x = Sales, fill = Region)) +
      geom_density(alpha = 0.5) +
      facet_wrap(~Region) +
      labs(title = "Comparative Sales Analysis by Region", x = "Sales Figures", y = "Density")
  
  # Descriptive statistics
  library(dplyr)
  summary_stats <- sales_data %>%
      group_by(Region) %>%
      summarise(Mean = mean(Sales), SD = sd(Sales), .groups = 'drop')
  print(summary_stats)
  ```

## Summary

This section not only reinforces statistical concepts through practical applications but also cultivates skills in data visualization and analysis. By engaging with real datasets and simulations, students enhance their ability to interpret and analyze data effectively, preparing them for professional roles that demand strong analytical capabilities.


# Exercises for Deepening Understanding

To solidify the concepts discussed in this module, the following exercises are designed to challenge students to apply theoretical knowledge in practical settings, ensuring a thorough understanding of key statistical principles.

## Exercise Details

### Exercise 1: Decision-Making Using Probability Distributions
- **Objective**: Apply different probability distributions to make informed decisions in hypothetical business scenarios.
- **Task**: Assume you're a manager at a retail company. Use the binomial distribution to determine the likelihood of at least 7 out of 10 customers purchasing a new product if the chance of one customer buying it is 30%. Discuss how this probability would influence a decision to increase product stock.
  
  **Example Solution**:
  ```r
  # Probability calculation using the binomial distribution
  success_prob <- pbinom(6, size=10, prob=0.3, lower.tail=FALSE)
  print(paste("Probability of at least 7 purchases:", success_prob))
  ```
```r
# TODO: Insert Exercise chunk with error messages and solutions.
```

### Exercise 2: Demonstrating the Central Limit Theorem
- **Objective**: Visualize and understand the Central Limit Theorem through simulation.
- **Task**: Simulate rolling a six-sided die 1000 times. Calculate the mean for each set of 50 rolls, and plot the distribution of these means. Analyze how the distribution changes with the number of rolls and discuss the implications of the CLT.
  
  **Example Solution**:
  ```r
  set.seed(123)  # For reproducibility
  roll_means <- replicate(1000, mean(sample(1:6, 50, replace=TRUE)))
  
  # Plotting the distribution of means
  ggplot(data.frame(Means = roll_means), aes(x = Means)) +
      geom_histogram(bins = 30, fill = "cornflowerblue") +
      ggtitle("Distribution of Means (Central Limit Theorem)")
  ```
```r
# TODO: Insert Exercise chunk with error messages and solutions.
```

### Exercise 3: Calculating Expected Values, Variance, and Applying the Law of Large Numbers
- **Objective**: Calculate expected values and variance, and observe the Law of Large Numbers in action.
- **Task**: Calculate the expected value and variance for the number of heads in 100 flips of a fair coin. Then simulate flipping the coin 1000 times and observe how the average number of heads stabilizes around the expected value.
  
  **Example Solution**:
  ```r
  # Expected value and variance calculation
  expected_heads <- 100 * 0.5
  variance_heads <- 100 * 0.5 * 0.5
  print(paste("Expected number of heads:", expected_heads))
  print(paste("Variance:", variance_heads))
  
  # Simulation to demonstrate the Law of Large Numbers
  coin_flips <- rbinom(1000, 100, 0.5)
  plot(cumsum(coin_flips) / 1:1000, type = "l", main = "Law of Large Numbers Demonstration",
       xlab = "Number of Trials", ylab = "Cumulative Average of Heads")
  abline(h = expected_heads / 100, col = "red")
  ```
  ```r
# TODO: Insert Exercise chunk with error messages and solutions.
```

## Summary

### Recap of Key Points
- We've explored how probability distributions can inform decision-making, demonstrated the Central Limit Theorem through simulations, and applied the Law of Large Numbers to observe the stability of averages over time.
- These exercises are integral in showing how theoretical statistical concepts apply in practical, real-world contexts.

### Further Reading and Resources
- **Books**: "Probability and Statistics" by Morris H. DeGroot and Mark J. Schervish offers comprehensive insights into the theories discussed.
- **Online Courses**: Platforms like Coursera and Khan Academy provide courses on statistics that further explore these topics.
- **Articles**: Research articles on the application of statistical methods in business and science can provide deeper insights into advanced topics.

By completing these exercises, students enhance their understanding of statistical principles and are better prepared to apply these concepts professionally and in the next chapters of this course.

<!--chapter:end:06-ProbabilityAndStatistics.Rmd-->

# Inferential Statistics and Hypothesis Testing

## Theoretical Basis for Hypothesis Testing and Significance

Inferential statistics allow researchers to make inferences about a population based on data collected from a sample. A core component of inferential statistics is hypothesis testing, which is a systematic method used to evaluate data and make decisions about a population parameter based on sample analysis.

## Null and Alternative Hypotheses

**Concepts**:
- **Null Hypothesis (H0)**: This hypothesis states that there is no significant difference or effect, and any observed difference is due to sampling or experimental error. It represents a statement of "no effect" or "no difference."
- **Alternative Hypothesis (H1 or Ha)**: This hypothesis is considered when the null hypothesis is rejected. It suggests that there is a true effect, and observed differences are not due to chance alone.

**Example in Context**:
Suppose a school claims their students' average test score is 75. To challenge this, we could set up:
- **H0**: The average score is 75 (µ = 75).
- **H1**: The average score is not 75 (µ ≠ 75).

## Type I and Type II Errors

**Definitions**:
- **Type I Error (α)**: Occurs when the null hypothesis is true, but we incorrectly reject it. It's often called a "false positive." The significance level (α), commonly set at 0.05, defines the probability of this error.
- **Type II Error (β)**: Happens when the null hypothesis is false, but we fail to reject it. This error is known as a "false negative." The power of the test (1 - β) measures the ability to avoid this error.

**Example in Practice**:
Using the school example, a Type I error would mean concluding that the average score is not 75 when it actually is 75. A Type II error would mean failing to reject the claim that the average is 75 when it is actually different.

## p-Values and Confidence Intervals

**Understanding p-Values**:
- A p-value is the probability of obtaining results at least as extreme as the observed results of a statistical hypothesis test, assuming that the null hypothesis is correct.
- A small p-value (≤ 0.05) indicates strong evidence against the null hypothesis, leading to its rejection.

**Confidence Intervals**:
- A confidence interval (CI) is a range of values that's used to estimate the true parameter of the population. For example, a 95% CI indicates that if the same population is sampled 100 times, approximately 95 of those confidence intervals will contain the true population parameter.
- CIs provide a measure of precision for an estimate.

**Demonstration with R**:

Suppose we have a sample of student scores from the school, and we want to test the claim:

```r
set.seed(123)
sample_scores <- rnorm(30, mean = 75, sd = 10)  # 30 sample scores with a mean of 75 and sd of 10

# Perform a One-Sample t-test
test_results <- t.test(sample_scores, mu = 75)

# Output the p-value and confidence interval
print(paste("p-value:", test_results$p.value))
print(paste("95% Confidence Interval:", paste(test_results$conf.int[1], test_results$conf.int[2], sep = " to ")))
```

This section provides a foundational understanding of hypothesis testing, equipping students with the knowledge to apply these concepts effectively in their own research and analysis. Through practical examples and demonstrations, students are encouraged to explore these statistical tools and understand their implications in real-world scenarios.


## Chi-Square, t-tests, z-tests, and Non-Parametric Tests

This section of the curriculum focuses on specific statistical tests used for hypothesis testing, each appropriate for different types of data and research questions. Understanding when and how to apply these tests is crucial for proper data analysis.

### Conducting and Interpreting Chi-Square Tests

**Purpose and Application**:
- The Chi-Square test is primarily used to determine whether there is a significant association between two categorical variables. It's often applied in market research, opinion polls, and educational research, among other fields.

**Example and Demonstration**:
- Consider a study wanting to explore if diet preference (vegetarian vs. non-vegetarian) is associated with gender among college students.

```r
# Sample data: Counts of male and female students preferring vegetarian and non-vegetarian diets
diet_data <- matrix(c(30, 70, 45, 55), nrow = 2,
                    dimnames = list(gender = c("Male", "Female"),
                                    diet = c("Vegetarian", "Non-Vegetarian")))

# Perform Chi-Square Test
chi_test <- chisq.test(diet_data)

# Output test results
print(chi_test)
```

### One-sample and Two-sample t-tests

**Purpose and Application**:
- **One-sample t-test**: Tests whether the mean of a single group differs from a specified mean.
- **Two-sample t-test** (independent samples): Tests whether the means of two groups are different.

**Example and Demonstration**:
- One-sample t-test: Testing if the average IQ of a sample of students is different from the national average IQ of 100.
- Two-sample t-test: Comparing the average test scores of two different classes.

```r
# One-sample t-test
iq_scores <- rnorm(25, mean = 102, sd = 15)  # Sample of 25 students
t_test_one <- t.test(iq_scores, mu = 100)

# Two-sample t-test
class1_scores <- rnorm(30, mean = 78, sd = 10)
class2_scores <- rnorm(30, mean = 85, sd = 10)
t_test_two <- t.test(class1_scores, class2_scores)

# Output test results
print(t_test_one)
print(t_test_two)
```

### Non-Parametric Alternatives to Parametric Tests

**Purpose and Application**:
- Non-parametric tests do not assume a specific distribution in the data and are useful when the assumptions for parametric tests (like normal distribution) are not met. These include tests like the Mann-Whitney U test, Wilcoxon Signed-Rank test, and Kruskal-Wallis test.

**Example and Demonstration**:
- Using the Mann-Whitney U test to compare the distributions of two groups' data that are not normally distributed.

```r
# Data: Test scores from two small classes
scores_class1 <- c(88, 82, 84, 91, 87, 85, 90)
scores_class2 <- c(78, 81, 79, 74, 80, 83, 77)

# Mann-Whitney U Test
mann_whitney_test <- wilcox.test(scores_class1, scores_class2)

# Output test results
print(mann_whitney_test)
```

## Exercises for Inferential Statistics and Hypothesis Testing

To consolidate the concepts learned in this chapter, the following exercises are designed to engage students in practical applications of inferential statistics methods. These activities will help students gain hands-on experience in hypothesis testing, understanding test assumptions, and interpreting results.

### Exercise 1: Conducting a Chi-Square Test
- **Objective**: Assess whether there is a significant relationship between two categorical variables.
- **Scenario**: A local library wants to determine if there is an association between gender and preference for fiction vs. non-fiction books. You are provided with the following data collected from a survey:
  - Males: 40 like fiction, 60 like non-fiction.
  - Females: 70 like fiction, 30 like non-fiction.
- **Task**: Perform a Chi-Square test to determine if gender is associated with book preference.

  **Steps**:
  ```r
  # Create the data matrix
  library_data <- matrix(c(40, 70, 60, 30), nrow = 2, byrow = TRUE,
                         dimnames = list(gender = c("Male", "Female"),
                                         preference = c("Fiction", "Non-Fiction")))
  
  # Conduct the Chi-Square test
  chi_results <- chisq.test(library_data)
  
  # Print the results
  print(chi_results)
  ```

### Exercise 2: One-sample t-test
- **Objective**: Test if the average measurement from a sample differs significantly from a known or hypothesized population mean.
- **Scenario**: An educational researcher claims that students on average spend 3.5 hours per day studying. You have data from a random sample of 25 students.
- **Task**: Perform a one-sample t-test to see if there is a significant difference from the national average.

  **Steps**:
  ```r
  # Sample data
  study_times <- rnorm(25, mean = 3.8, sd = 1)  # higher mean suggests different studying habits
  
  # Perform the t-test
  t_results <- t.test(study_times, mu = 3.5)
  
  # Output the results
  print(t_results)
  ```
  ```r
# TODO: Insert Exercise chunk with error messages and solutions.
```

### Exercise 3: Exploring Non-Parametric Tests
- **Objective**: Use non-parametric methods to test hypotheses when data do not meet the assumptions required for parametric tests.
- **Scenario**: You suspect that two groups of plants have different growth rates, but the data are not normally distributed.
- **Task**: Apply the Mann-Whitney U test to compare the growth rates of the two groups.

  **Steps**:
  ```r
  # Plant growth data (non-normal distribution assumed)
  growth_group1 <- c(2, 3, 1, 4, 2, 1, 3)
  growth_group2 <- c(3, 4, 6, 5, 6)
  
  # Mann-Whitney U Test
  growth_test <- wilcox.test(growth_group1, growth_group2)
  
  # Print the test results
  print(growth_test)
  ```
  ```r
# TODO: Insert Exercise chunk with error messages and solutions.
```

<!--chapter:end:07-InferentialAndHypothesis.Rmd-->

# Analysis of Variance (ANOVA)

## Conducting and Interpreting One-Way and Two-Way ANOVA

Analysis of Variance (ANOVA) is a statistical method used to compare the means of three or more groups to see if at least one of them is significantly different from the others. It is an extension of the t-test that allows for the comparison of multiple groups.

### Assumptions of ANOVA
Before performing ANOVA, the following assumptions must be met:
- **Normality**: The data in each group should be approximately normally distributed.
- **Homogeneity of variances**: The variances among the groups should be approximately equal.
- **Independence**: The observations should be independent of each other.

### Interpreting ANOVA Tables
The ANOVA table breaks down the sources of variation in the data into between-group and within-group variations. It shows the sums of squares, mean squares, F-values, and p-values. A significant p-value (typically ≤ 0.05) indicates that there are significant differences among the group means.

### Post Hoc Tests and Multiple Comparisons
When ANOVA shows significant results, post hoc tests help identify which specific groups differ from each other. Common post hoc tests include Tukey's HSD, Bonferroni, and Scheffé tests.

**Example and Demonstration**:

```r
# Example data
group1 <- rnorm(20, mean = 5)
group2 <- rnorm(20, mean = 6)
group3 <- rnorm(20, mean = 7)

data <- data.frame(
  value = c(group1, group2, group3),
  group = factor(rep(c("Group1", "Group2", "Group3"), each = 20))
)

# Perform One-Way ANOVA
anova_result <- aov(value ~ group, data = data)

# Output ANOVA table
summary(anova_result)

# Perform post hoc test
tukey_result <- TukeyHSD(anova_result)
print(tukey_result)
```

## Understanding the F-test and its Applications

The F-test is used in ANOVA to compare the variances of two or more groups and determine if there are significant differences among the group means.

### The F Distribution and the F Ratio
- The F distribution is used as the null distribution for the test statistic in ANOVA.
- The F ratio is the ratio of between-groups variance to within-groups variance.

### Between-groups and Within-groups Variance
- **Between-groups variance**: Measures the variability due to the differences between group means.
- **Within-groups variance**: Measures the variability within each group.

### Applications in Experimental Design
The F-test in ANOVA helps determine if the means of multiple groups are different. It is also used in regression analysis to test the overall significance of the model.

**Example and Demonstration**:

```r
# Example data for F-test
# This example will be part of the exercises.
```

# Exercises for ANOVA

To consolidate the concepts learned in this chapter, the following exercises are designed to engage students in practical applications of ANOVA. These activities will help students gain hands-on experience in conducting and interpreting ANOVA tests, understanding test assumptions, and performing post hoc analyses.

## Exercise 1: Conducting a One-Way ANOVA
- **Objective**: Assess whether there is a significant difference in the means of three groups.
- **Scenario**: A researcher is studying the effect of three different diets on weight loss. The data collected includes weight loss measurements for three groups following different diets.
- **Task**: Perform a One-Way ANOVA to determine if there is a significant difference in weight loss among the three diet groups.

  **Steps**:
  ```r
  # Sample data: Weight loss measurements for three diet groups
  diet1 <- c(2.1, 2.3, 1.8, 2.5, 2.0)
  diet2 <- c(2.0, 1.9, 2.2, 2.1, 1.8)
  diet3 <- c(1.7, 1.8, 1.6, 1.9, 1.5)

  weight_loss <- data.frame(
    loss = c(diet1, diet2, diet3),
    diet = factor(rep(c("Diet1", "Diet2", "Diet3"), each = 5))
  )

  # Perform One-Way ANOVA
  anova_result <- aov(loss ~ diet, data = weight_loss)
  summary(anova_result)

  # Perform post hoc test
  tukey_result <- TukeyHSD(anova_result)
  print(tukey_result)
  ```
  ```r
  # TODO: Insert Exercise chunk with error messages and solutions.
  ```

## Exercise 2: Conducting a Two-Way ANOVA
- **Objective**: Assess the interaction effects of two factors on a response variable.
- **Scenario**: A researcher is studying the effects of different fertilizers and watering schedules on plant growth. The data collected includes growth measurements for plants under different fertilizer and watering conditions.
- **Task**: Perform a Two-Way ANOVA to determine the main effects and interaction effects of fertilizer and watering on plant growth.

  **Steps**:
  ```r
  # Sample data: Plant growth measurements for different fertilizer and watering conditions
  growth <- c(20, 22, 19, 24, 23, 25, 18, 21, 22, 23, 26, 24, 22, 21, 20)
  fertilizer <- factor(rep(c("Fertilizer1", "Fertilizer2", "Fertilizer3"), each = 5))
  watering <- factor(rep(c("Watering1", "Watering2", "Watering3"), times = 5))

  plant_growth <- data.frame(growth, fertilizer, watering)

  # Perform Two-Way ANOVA
  anova_result <- aov(growth ~ fertilizer * watering, data = plant_growth)
  summary(anova_result)
  ```
  ```r
  # TODO: Insert Exercise chunk with error messages and solutions.
  ```

## Exercise 3: Understanding the F-test in Regression Analysis
- **Objective**: Use the F-test to assess the overall significance of a regression model.
- **Scenario**: A researcher is building a regression model to predict house prices based on square footage and the number of bedrooms.
- **Task**: Perform an F-test to determine if the regression model significantly predicts house prices.

  **Steps**:
  ```r
  # Sample data: House prices with square footage and number of bedrooms
  prices <- c(300000, 350000, 400000, 250000, 450000, 500000)
  sqft <- c(1500, 1600, 1700, 1400, 1800, 2000)
  bedrooms <- c(3, 3, 4, 2, 4, 5)

  house_data <- data.frame(prices, sqft, bedrooms)

  # Build regression model
  model <- lm(prices ~ sqft + bedrooms, data = house_data)

  # Perform F-test
  summary(model)
  ```
  ```r
  # TODO: Insert Exercise chunk with error messages and solutions.
  ```

## Exercise 4: Conducting Post Hoc Tests
- **Objective**: Use post hoc tests to determine which specific groups differ after a significant ANOVA result.
- **Scenario**: After finding a significant difference in mean test scores among different teaching methods, a researcher wants to know which teaching methods are significantly different from each other.
- **Task**: Perform a post hoc test to determine the specific differences between teaching methods.

  **Steps**:
  ```r
  # Sample data: Test scores for different teaching methods
  method1 <- c(85, 87, 88, 90, 86)
  method2 <- c(80, 82, 81, 79, 83)
  method3 <- c(88, 90, 89, 87, 91)

  test_scores <- data.frame(
    score = c(method1, method2, method3),
    method = factor(rep(c("Method1", "Method2", "Method3"), each = 5))
  )

  # Perform One-Way ANOVA
  anova_result <- aov(score ~ method, data = test_scores)
  summary(anova_result)

  # Perform post hoc test
  tukey_result <- TukeyHSD(anova_result)
  print(tukey_result)
  ```

  ```r
  # TODO: Insert Exercise chunk with error messages and solutions.
  ```

<!--chapter:end:08-ANOVA.Rmd-->

# Regression Analysis

## Simple and Multiple Linear Regression (OLS, MLR)

Regression analysis is used to understand the relationship between a dependent variable and one or more independent variables. It helps in predicting the value of the dependent variable based on the values of the independent variables.

### Constructing and Fitting Regression Models
- **Simple Linear Regression**: This involves a single independent variable. The relationship between the dependent variable $y$ and the independent variable $x$ is modeled as $y = \beta_0 + \beta_1 x + \epsilon$.
- **Multiple Linear Regression**: This involves multiple independent variables. The relationship is modeled as $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p + \epsilon$.

### Understanding Coefficients and Predictions
- **Coefficients**: The coefficients $\beta_i$ represent the change in the dependent variable for a one-unit change in the independent variable.
- **Predictions**: Once the model is fitted, it can be used to predict the values of the dependent variable for given values of the independent variables.

### Multiple Regression and Adjusting for Confounders
- **Confounders**: Variables that influence both the dependent and independent variables. Multiple regression helps in adjusting for these confounders by including them in the model.

**Example and Demonstration**:

```r
# Example data
set.seed(123)
x1 <- rnorm(100, mean = 5)
x2 <- rnorm(100, mean = 7)
y <- 3 + 2*x1 + 1.5*x2 + rnorm(100)

data <- data.frame(y, x1, x2)

# Fit Simple Linear Regression
model_simple <- lm(y ~ x1, data = data)
summary(model_simple)

# Fit Multiple Linear Regression
model_multiple <- lm(y ~ x1 + x2, data = data)
summary(model_multiple)
```

## Diagnostics and Assumptions of Linear Models

### Residual Analysis and Model Fit
- **Residuals**: The differences between the observed and predicted values. Residual analysis helps in checking the assumptions of the model.
- **Model Fit**: Assessed using various statistics like R-squared, which indicates the proportion of variance explained by the model.

### Checking for Heteroscedasticity and Multicollinearity
- **Heteroscedasticity**: Occurs when the variance of the residuals is not constant. This can be checked using plots of residuals vs fitted values.
- **Multicollinearity**: Occurs when independent variables are highly correlated. This can be checked using the Variance Inflation Factor (VIF).

### Model Selection Criteria (AIC, BIC, R-squared)
- **AIC (Akaike Information Criterion)**: A measure of the relative quality of a statistical model for a given set of data.
- **BIC (Bayesian Information Criterion)**: Similar to AIC but with a larger penalty for models with more parameters.
- **R-squared**: Indicates the proportion of variance in the dependent variable that is predictable from the independent variables.

**Example and Demonstration**:

```r
# Residual analysis
par(mfrow = c(2, 2))
plot(model_multiple)

# Check for heteroscedasticity
library(car)
ncvTest(model_multiple)

# Check for multicollinearity
vif(model_multiple)
```

# Exercises for Regression Analysis

## Exercise 1: Simple Linear Regression
- **Objective**: Fit a simple linear regression model and interpret the results.
- **Scenario**: A researcher wants to study the relationship between advertising spend and sales.
- **Task**: Perform a simple linear regression with sales as the dependent variable and advertising spend as the independent variable.

  **Steps**:
  ```r
  # Sample data: Advertising spend and sales
  advertising <- c(230, 150, 300, 290, 180, 270, 320, 210, 250, 190)
  sales <- c(480, 340, 600, 590, 380, 540, 640, 410, 500, 430)

  data <- data.frame(sales, advertising)

  # Fit Simple Linear Regression
  model_simple <- lm(sales ~ advertising, data = data)
  summary(model_simple)
  ```
  ```r
  # TODO: Insert Exercise chunk with error messages and solutions.
  ```

## Exercise 2: Multiple Linear Regression
- **Objective**: Fit a multiple linear regression model and interpret the results.
- **Scenario**: A researcher wants to predict house prices based on square footage and number of bedrooms.
- **Task**: Perform a multiple linear regression with house prices as the dependent variable and square footage and number of bedrooms as independent variables.

  **Steps**:
  ```r
  # Sample data: House prices, square footage, and number of bedrooms
  prices <- c(300000, 350000, 400000, 250000, 450000, 500000)
  sqft <- c(1500, 1600, 1700, 1400, 1800, 2000)
  bedrooms <- c(3, 3, 4, 2, 4, 5)

  house_data <- data.frame(prices, sqft, bedrooms)

  # Fit Multiple Linear Regression
  model_multiple <- lm(prices ~ sqft + bedrooms, data = house_data)
  summary(model_multiple)
  ```
  ```r
  # TODO: Insert Exercise chunk with error messages and solutions.
  ```

## Exercise 3: Residual Analysis
- **Objective**: Perform residual analysis to check the assumptions of the regression model.
- **Scenario**: A researcher has fitted a multiple linear regression model and wants to validate its assumptions.
- **Task**: Analyze the residuals of the model and check for heteroscedasticity and multicollinearity.

  **Steps**:
  ```r
  # Residual analysis
  par(mfrow = c(2, 2))
  plot(model_multiple)

  # Check for heteroscedasticity
  library(car)
  ncvTest(model_multiple)

  # Check for multicollinearity
  vif(model_multiple)
  ```
  ```r
  # TODO: Insert Exercise chunk with error messages and solutions.
  ```

## Exercise 4: Model Selection Criteria
- **Objective**: Compare different models using AIC and BIC.
- **Scenario**: A researcher has two different regression models and wants to determine which one is better.
- **Task**: Calculate and compare the AIC and BIC values for both models.

  **Steps**:
  ```r
  # Model 1: Simple linear regression
  model1 <- lm(prices ~ sqft, data = house_data)

  # Model 2: Multiple linear regression
  model2 <- lm(prices ~ sqft + bedrooms, data = house_data)

  # Compare AIC and BIC
  AIC(model1, model2)
  BIC(model1, model2)
  ```

  ```r
  # TODO: Insert Exercise chunk with error messages and solutions.
  ```

<!--chapter:end:09-RegressionAnalysis.Rmd-->

# Categorical Data Analysis

## Introduction to Logistic Regression

Logistic regression is used when the dependent variable is binary (i.e., it has two possible outcomes). It models the probability of the occurrence of an event by fitting data to a logistic curve.

### Odds Ratios and Logit Function
- **Odds Ratios**: The odds ratio is a measure of association between an exposure and an outcome. It represents the odds that an event will occur given a particular exposure, compared to the odds of the event occurring without that exposure.
- **Logit Function**: The logit function is the natural logarithm of the odds of the dependent event. Logistic regression uses this function to model the relationship between the independent variables and the probability of the dependent event occurring.

### Model Fitting and Interpretation
- **Model Fitting**: Logistic regression models are fitted using maximum likelihood estimation. The coefficients obtained from the model represent the change in the log odds of the dependent variable for a one-unit change in the independent variable.
- **Interpretation**: The exponentiated coefficients (exp(coef)) can be interpreted as odds ratios. A coefficient of 0 means no effect, positive coefficients increase the odds, and negative coefficients decrease the odds.

**Example and Demonstration**:

```r
# Example data
set.seed(123)
x1 <- rnorm(100)
x2 <- rnorm(100)
y <- rbinom(100, 1, prob = 1 / (1 + exp(-(0.5 + 1.5 * x1 - 1 * x2))))

data <- data.frame(y, x1, x2)

# Fit Logistic Regression
model_logit <- glm(y ~ x1 + x2, data = data, family = binomial)
summary(model_logit)

# Interpretation of coefficients
exp(coef(model_logit))
```

### Assessing Model Goodness of Fit
- **Goodness of Fit**: The goodness of fit of a logistic regression model can be assessed using various methods such as the Hosmer-Lemeshow test, Akaike Information Criterion (AIC), and analysis of residuals.
- **Example**: Use the Hosmer-Lemeshow test to assess the model fit.

```r
# Assessing model goodness of fit
library(ResourceSelection)
hoslem.test(data$y, fitted(model_logit))
```

## Modeling and Interpretation of Binary Outcomes

### Predictive Modeling with Binary Data
Logistic regression is commonly used for predictive modeling with binary outcomes. The model predicts the probability that a given input belongs to one of the two outcome categories.

### Evaluating Model Performance (ROC Curves, AUC)
- **ROC Curve**: A Receiver Operating Characteristic (ROC) curve is a graphical representation of a model's diagnostic ability. It plots the true positive rate (sensitivity) against the false positive rate (1 - specificity) for different threshold values.
- **AUC (Area Under the Curve)**: The AUC is a single scalar value that summarizes the performance of the model across all threshold values. A higher AUC indicates better model performance.

**Example and Demonstration**:

```r
# ROC Curve and AUC
library(pROC)
roc_curve <- roc(data$y, fitted(model_logit))
plot(roc_curve)
auc(roc_curve)
```

# Exercises for Categorical Data Analysis

## Exercise 1: Logistic Regression
- **Objective**: Fit a logistic regression model and interpret the results.
- **Scenario**: A researcher wants to study the effect of two variables on the likelihood of an event occurring.
- **Task**: Perform a logistic regression with a binary outcome variable and two predictors.

  **Steps**:
  ```r
  # Sample data
  x1 <- rnorm(100)
  x2 <- rnorm(100)
  y <- rbinom(100, 1, prob = 1 / (1 + exp(-(0.5 + 1.5 * x1 - 1 * x2))))

  data <- data.frame(y, x1, x2)

  # Fit Logistic Regression
  model_logit <- glm(y ~ x1 + x2, data = data, family = binomial)
  summary(model_logit)

  # Interpretation of coefficients
  exp(coef(model_logit))
  ```
  ```r
  # TODO: Insert Exercise chunk with error messages and solutions.
  ```

## Exercise 2: Model Goodness of Fit
- **Objective**: Assess the goodness of fit of a logistic regression model.
- **Scenario**: A researcher has fitted a logistic regression model and wants to validate its fit.
- **Task**: Use the Hosmer-Lemeshow test to assess the model fit.

  **Steps**:
  ```r
  # Assessing model goodness of fit
  library(ResourceSelection)
  hoslem.test(data$y, fitted(model_logit))
  ```
  ```r
  # TODO: Insert Exercise chunk with error messages and solutions.
  ```

## Exercise 3: Evaluating Model Performance
- **Objective**: Evaluate the performance of a logistic regression model using ROC curves and AUC.
- **Scenario**: A researcher wants to evaluate the performance of their logistic regression model.
- **Task**: Plot the ROC curve and calculate the AUC for the logistic regression model.

  **Steps**:
  ```r
  # ROC Curve and AUC
  library(pROC)
  roc_curve <- roc(data$y, fitted(model_logit))
  plot(roc_curve)
  auc(roc_curve)
  ```
  ```r
  # TODO: Insert Exercise chunk with error messages and solutions.
  ```

<!--chapter:end:10-CategoricalDataAnalysis.Rmd-->

# Code Challenge 1: Fundamentals and Data Structures (After Module 02)

**Objective**: Test students on basic R programming, data structures, and initial data handling techniques.

**Modules Covered**: 01-R-Fundamentals.Rmd, 02-DataStructureHandling.Rmd

- ## Instructions:

1. **Basic R Programming**:
    - **Task 1**: Write a function to calculate the factorial of a number using both iterative and recursive methods.
    - **Task 2**: Demonstrate the use of control structures (if-else, for loop) to manipulate a vector of numbers.

2. **Data Structures**:
    - **Task 3**: Create and manipulate vectors, matrices, lists, and data frames.
    - **Task 4**: Convert a list into a data frame and perform basic data operations (e.g., selecting columns, filtering rows).

3. **Basic Data Handling**:
    - **Task 5**: Load and explore a dataset from R's built-in datasets (e.g., `iris`).
    - **Task 6**: Summarize the data (mean, median, mode) and visualize basic distributions using histograms and box plots.

- ## Template

```r
# CODE CHALLENGE 1 TEMPLATE

# STUDENT NAME: ______________________
# STUDENT ID: ________________________

########################
# SECTION 1: Basic R Programming
########################

# Task 1: Write a function to calculate the factorial of a number using both iterative and recursive methods.

# Iterative method
factorial_iterative <- function(n) {
  result <- 1
  for (i in 1:n) {
    result <- result * i
  }
  return(result)
}

# Recursive method
factorial_recursive <- function(n) {
  if (n == 0) {
    return(1)
  } else {
    return(n * factorial_recursive(n - 1))
  }
}

# Test the functions
print(factorial_iterative(5)) # Expected output: 120
print(factorial_recursive(5)) # Expected output: 120

# Task 2: Demonstrate the use of control structures (if-else, for loop) to manipulate a vector of numbers.

# Create a vector of numbers
numbers <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)

# Use if-else and for loop to manipulate the vector
result_vector <- c()
for (num in numbers) {
  if (num %% 2 == 0) {
    result_vector <- c(result_vector, num^2) # Square the even numbers
  } else {
    result_vector <- c(result_vector, num^3) # Cube the odd numbers
  }
}
print(result_vector) # Expected output: [1]  1  4 27 16 125 36 343 64 729 100

########################
# SECTION 2: Data Structures
########################

# Task 3: Create and manipulate vectors, matrices, lists, and data frames.

# Vector
vector <- c(1, 2, 3, 4, 5)
print(vector) # Expected output: [1] 1 2 3 4 5

# Matrix
matrix_data <- matrix(1:9, nrow = 3, ncol = 3)
print(matrix_data) # Expected output: 3x3 matrix with values from 1 to 9

# List
my_list <- list(name = "John", age = 25, scores = c(90, 85, 88))
print(my_list) # Expected output: list with name, age, and scores

# Data Frame
data_frame <- data.frame(name = c("John", "Jane", "Doe"), age = c(25, 30, 22), score = c(90, 85, 88))
print(data_frame) # Expected output: data frame with name, age, and score columns

# Task 4: Convert a list into a data frame and perform basic data operations (e.g., selecting columns, filtering rows).

# Convert list to data frame
list_to_df <- as.data.frame(my_list$scores)
names(list_to_df) <- "scores"
print(list_to_df) # Expected output: data frame with scores column

# Select columns
selected_columns <- data_frame[, c("name", "score")]
print(selected_columns) # Expected output: data frame with name and score columns

# Filter rows
filtered_rows <- data_frame[data_frame$age > 25, ]
print(filtered_rows) # Expected output: data frame with rows where age > 25

########################
# SECTION 3: Basic Data Handling
########################

# Task 5: Load and explore a dataset from R's built-in datasets (e.g., iris).

# Load the iris dataset
data(iris)
print(head(iris)) # Expected output: first 6 rows of the iris dataset

# Task 6: Summarize the data (mean, median, mode) and visualize basic distributions using histograms and box plots.

# Summary statistics
mean_values <- colMeans(iris[, 1:4])
median_values <- apply(iris[, 1:4], 2, median)

# Custom function to calculate mode
calculate_mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}
mode_values <- sapply(iris[, 1:4], calculate_mode)

print(mean_values) # Expected output: mean of each numeric column
print(median_values) # Expected output: median of each numeric column
print(mode_values) # Expected output: mode of each numeric column

# Visualization
library(ggplot2)

# Histograms
ggplot(iris, aes(x = Sepal.Length)) + 
  geom_histogram(binwidth = 0.5, fill = "blue", color = "black") +
  ggtitle("Histogram of Sepal Length")

# Box Plots
ggplot(iris, aes(x = Species, y = Sepal.Length)) + 
  geom_boxplot(fill = "lightblue", color = "black") +
  ggtitle("Box Plot of Sepal Length by Species")
```

- ## Submission Instructions:

1. Complete the code challenge template by filling in your solutions in the specified sections.
2. Save your completed R script with the filename format: `CodeChallenge1_YourName.R`
3. Upload your script to your GitHub repository under the Code Challenges folder.
4. The auto-grader will evaluate your submission and provide immediate feedback.
5. Once you are satisfied with your marks, finalize your submission which will automatically be sent to the instructor's Google Drive for review.

<!--chapter:end:11-CodeChallenge-1.Rmd-->

# Code Challenge 2: Descriptive Statistics, Probability, and Inferential Statistics (After Module 07)

**Objective**: Test students on their understanding of descriptive statistics, probability distributions, and inferential statistics.

**Modules Covered**: 05-DescriptiveStatistics.Rmd, 06-ProbabilityAndStatistics.Rmd, 07-InferentialAndHypothesis.Rmd

- ## Instructions:

1. **Descriptive Statistics**:
    - **Task 1**: Calculate the mean, median, mode, variance, and standard deviation of a given dataset.
    - **Task 2**: Normalize and standardize the dataset.
    - **Task 3**: Create histograms and box plots to visualize the data distribution.

2. **Probability Distributions**:
    - **Task 4**: Demonstrate the use of binomial, Poisson, and normal distributions.
    - **Task 5**: Simulate the Central Limit Theorem (CLT) using a random dataset.

3. **Inferential Statistics**:
    - **Task 6**: Conduct a chi-square test to assess the relationship between two categorical variables.
    - **Task 7**: Perform a one-sample t-test to compare a sample mean to a known population mean.
    - **Task 8**: Use a non-parametric test (Mann-Whitney U test) to compare two groups with non-normally distributed data.

- ## Template:

```r
# CODE CHALLENGE 2 TEMPLATE

# STUDENT NAME: ______________________
# STUDENT ID: ________________________

########################
# SECTION 1: Descriptive Statistics
########################

# Task 1: Calculate the mean, median, mode, variance, and standard deviation of a given dataset.

# Sample data
data <- c(22, 24, 24, 18, 30, 32, 19, 21, 24, 20, 23, 19, 22, 25)

# Calculating mean
mean_value <- mean(data)
print(paste("Mean:", mean_value))

# Calculating median
median_value <- median(data)
print(paste("Median:", median_value))

# Calculating mode
get_mode <- function(x) {
  uniqx <- unique(x)
  uniqx[which.max(tabulate(match(x, uniqx)))]
}
mode_value <- get_mode(data)
print(paste("Mode:", mode_value))

# Calculating variance
variance_value <- var(data)
print(paste("Variance:", variance_value))

# Calculating standard deviation
sd_value <- sd(data)
print(paste("Standard Deviation:", sd_value))

# Task 2: Normalize and standardize the dataset.

# Normalization
normalize <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}
normalized_data <- normalize(data)
print("Normalized Data:")
print(normalized_data)

# Standardization
standardize <- function(x) {
  (x - mean(x)) / sd(x)
}
standardized_data <- standardize(data)
print("Standardized Data:")
print(standardized_data)

# Task 3: Create histograms and box plots to visualize the data distribution.

library(ggplot2)

# Histogram
ggplot(data.frame(data), aes(x = data)) +
  geom_histogram(binwidth = 1, fill = 'blue', color = 'black') +
  ggtitle("Histogram of Data")

# Box Plot
ggplot(data.frame(data), aes(y = data)) +
  geom_boxplot(fill = 'tomato') +
  ggtitle("Box Plot of Data")

########################
# SECTION 2: Probability Distributions
########################

# Task 4: Demonstrate the use of binomial, Poisson, and normal distributions.

# Binomial example: Probability of exactly 6 heads in 10 fair coin tosses
binom_prob <- dbinom(6, size = 10, prob = 0.5)
print(paste("Binomial Probability (6 heads out of 10):", binom_prob))

# Poisson example: Probability of receiving 3 emails in an hour if the average rate is 5 emails per hour
pois_prob <- dpois(3, lambda = 5)
print(paste("Poisson Probability (3 emails):", pois_prob))

# Normal distribution example: Probability density of a score at the mean of a distribution
norm_prob <- dnorm(0, mean = 0, sd = 1)
print(paste("Normal Distribution Density (mean):", norm_prob))

# Task 5: Simulate the Central Limit Theorem (CLT) using a random dataset.

# Simulating sample means
sample_means <- replicate(1000, mean(runif(50, min = 0, max = 1)))
hist(sample_means, probability = TRUE, main = "CLT Simulation")
lines(density(sample_means), col = "red")

########################
# SECTION 3: Inferential Statistics
########################

# Task 6: Conduct a chi-square test to assess the relationship between two categorical variables.

# Sample data: Counts of male and female students preferring vegetarian and non-vegetarian diets
diet_data <- matrix(c(30, 70, 45, 55), nrow = 2,
                    dimnames = list(gender = c("Male", "Female"),
                                    diet = c("Vegetarian", "Non-Vegetarian")))

# Perform Chi-Square Test
chi_test <- chisq.test(diet_data)
print(chi_test)

# Task 7: Perform a one-sample t-test to compare a sample mean to a known population mean.

# Sample data
study_times <- rnorm(25, mean = 3.8, sd = 1)

# Perform the t-test
t_test_results <- t.test(study_times, mu = 3.5)
print(t_test_results)

# Task 8: Use a non-parametric test (Mann-Whitney U test) to compare two groups with non-normally distributed data.

# Data: Test scores from two small classes
scores_class1 <- c(88, 82, 84, 91, 87, 85, 90)
scores_class2 <- c(78, 81, 79, 74, 80, 83, 77)

# Mann-Whitney U Test
mann_whitney_test <- wilcox.test(scores_class1, scores_class2)
print(mann_whitney_test)
```

- ## Submission Instructions:

1. Complete the code challenge template by filling in your solutions in the specified sections.
2. Save your completed R script with the filename format: `CodeChallenge2_YourName.R`
3. Upload your script to your GitHub repository under the Code Challenges folder.
4. The auto-grader will evaluate your submission and provide immediate feedback.
5. Once you are satisfied with your marks, finalize your submission which will automatically be sent to the instructor's Google Drive for review.

<!--chapter:end:12-CodeChallenge-2.Rmd-->

# Code Challenge 3: ANOVA, Regression Analysis, and Logistic Regression (After Module 10)

**Objective**: Test students on their understanding of ANOVA, regression analysis, and logistic regression.

**Modules Covered**: 08-ANOVA.Rmd, 09-RegressionAnalysis.Rmd, 10-CategoricalDataAnalysis.Rmd

- ## Instructions:

1. **ANOVA**:
    - **Task 1**: Conduct a one-way ANOVA to determine if there are significant differences in group means.
    - **Task 2**: Perform a two-way ANOVA to analyze the interaction effects of two factors on a response variable.

2. **Regression Analysis**:
    - **Task 3**: Fit a simple linear regression model and interpret the results.
    - **Task 4**: Fit a multiple linear regression model, perform residual analysis, and check for heteroscedasticity and multicollinearity.

3. **Logistic Regression**:
    - **Task 5**: Fit a logistic regression model with a binary outcome and interpret the results.
    - **Task 6**: Evaluate the logistic regression model performance using ROC curves and AUC.

- ## Template:

```r
# CODE CHALLENGE 3 TEMPLATE

# STUDENT NAME: ______________________
# STUDENT ID: ________________________

########################
# SECTION 1: ANOVA
########################

# Task 1: Conduct a one-way ANOVA to determine if there are significant differences in group means.

# Sample data: Weight loss measurements for three diet groups
diet1 <- c(2.1, 2.3, 1.8, 2.5, 2.0)
diet2 <- c(2.0, 1.9, 2.2, 2.1, 1.8)
diet3 <- c(1.7, 1.8, 1.6, 1.9, 1.5)

weight_loss <- data.frame(
  loss = c(diet1, diet2, diet3),
  diet = factor(rep(c("Diet1", "Diet2", "Diet3"), each = 5))
)

# Perform One-Way ANOVA
anova_result <- aov(loss ~ diet, data = weight_loss)
summary(anova_result)

# Perform post hoc test
tukey_result <- TukeyHSD(anova_result)
print(tukey_result)

# Task 2: Perform a two-way ANOVA to analyze the interaction effects of two factors on a response variable.

# Sample data: Plant growth measurements for different fertilizer and watering conditions
growth <- c(20, 22, 19, 24, 23, 25, 18, 21, 22, 23, 26, 24, 22, 21, 20)
fertilizer <- factor(rep(c("Fertilizer1", "Fertilizer2", "Fertilizer3"), each = 5))
watering <- factor(rep(c("Watering1", "Watering2", "Watering3"), times = 5))

plant_growth <- data.frame(growth, fertilizer, watering)

# Perform Two-Way ANOVA
anova_result_2way <- aov(growth ~ fertilizer * watering, data = plant_growth)
summary(anova_result_2way)

########################
# SECTION 2: Regression Analysis
########################

# Task 3: Fit a simple linear regression model and interpret the results.

# Sample data: Advertising spend and sales
advertising <- c(230, 150, 300, 290, 180, 270, 320, 210, 250, 190)
sales <- c(480, 340, 600, 590, 380, 540, 640, 410, 500, 430)

data <- data.frame(sales, advertising)

# Fit Simple Linear Regression
model_simple <- lm(sales ~ advertising, data = data)
summary(model_simple)

# Task 4: Fit a multiple linear regression model, perform residual analysis, and check for heteroscedasticity and multicollinearity.

# Sample data: House prices, square footage, and number of bedrooms
prices <- c(300000, 350000, 400000, 250000, 450000, 500000)
sqft <- c(1500, 1600, 1700, 1400, 1800, 2000)
bedrooms <- c(3, 3, 4, 2, 4, 5)

house_data <- data.frame(prices, sqft, bedrooms)

# Fit Multiple Linear Regression
model_multiple <- lm(prices ~ sqft + bedrooms, data = house_data)
summary(model_multiple)

# Residual analysis
par(mfrow = c(2, 2))
plot(model_multiple)

# Check for heteroscedasticity
library(car)
ncvTest(model_multiple)

# Check for multicollinearity
vif(model_multiple)

########################
# SECTION 3: Logistic Regression
########################

# Task 5: Fit a logistic regression model with a binary outcome and interpret the results.

# Sample data
set.seed(123)
x1 <- rnorm(100)
x2 <- rnorm(100)
y <- rbinom(100, 1, prob = 1 / (1 + exp(-(0.5 + 1.5 * x1 - 1 * x2))))

data_logit <- data.frame(y, x1, x2)

# Fit Logistic Regression
model_logit <- glm(y ~ x1 + x2, data = data_logit, family = binomial)
summary(model_logit)

# Interpretation of coefficients
exp(coef(model_logit))

# Task 6: Evaluate the logistic regression model performance using ROC curves and AUC.

# ROC Curve and AUC
library(pROC)
roc_curve <- roc(data_logit$y, fitted(model_logit))
plot(roc_curve)
auc(roc_curve)
```

- ## Submission Instructions:

1. Complete the code challenge template by filling in your solutions in the specified sections.
2. Save your completed R script with the filename format: `CodeChallenge3_YourName.R`
3. Upload your script to your GitHub repository under the Code Challenges folder.
4. The auto-grader will evaluate your submission and provide immediate feedback.
5. Once you are satisfied with your marks, finalize your submission which will automatically be sent to the instructor's Google Drive for review.

<!--chapter:end:13-CodeChallenge-3.Rmd-->

# Advanced Data Manipulation and Transformation (`OPTIONAL`)

In advanced data analysis, efficiently manipulating and transforming data is crucial. The `tidyverse` package in R provides a powerful and coherent set of tools for data wrangling.

## Tidyverse Approaches to Data Wrangling

The `tidyverse` is a collection of R packages designed for data science. It includes `dplyr` for data manipulation, `tidyr` for data tidying, and `ggplot2` for data visualization.

### Key Functions in `dplyr`
- **`select()`**: Select specific columns.
- **`filter()`**: Filter rows based on conditions.
- **`mutate()`**: Create new columns or modify existing ones.
- **`summarise()`**: Summarize data (e.g., calculating means or counts).
- **`group_by()`**: Group data by one or more variables.

**Example**:

```r
library(tidyverse)

# Load the dataset
data <- iris

# Select specific columns
selected_data <- data %>%
  select(Sepal.Length, Species)

# Filter rows based on condition
filtered_data <- data %>%
  filter(Sepal.Length > 5)

# Create a new column
mutated_data <- data %>%
  mutate(Sepal.Ratio = Sepal.Length / Sepal.Width)

# Summarize data
summary_data <- data %>%
  group_by(Species) %>%
  summarise(Mean_Sepal.Length = mean(Sepal.Length),
            Count = n())
```

## Dealing with Text and Categorical Data

Handling text and categorical data involves operations like re-coding factors, string manipulation, and one-hot encoding.

### Key Functions
- **`factor()`**: Create or modify factors.
- **`str_*` functions**: String manipulation functions from the `stringr` package (part of `tidyverse`).

**Example**:

```r
# Create a factor variable
data$Species <- factor(data$Species, levels = c("setosa", "versicolor", "virginica"))

# String manipulation
library(stringr)
text_data <- c("apple", "banana", "cherry")
text_lengths <- str_length(text_data)
text_upper <- str_to_upper(text_data)
```

## Working with Dates and Times

Handling dates and times is critical in many data analyses. The `lubridate` package (part of the `tidyverse`) makes it easier to work with date-time data.

### Key Functions
- **`ymd()`, `mdy()`, `dmy()`**: Parse dates in different formats.
- **`year()`, `month()`, `day()`**: Extract components of dates.
- **`now()`**: Get the current date and time.

**Example**:

```r
library(lubridate)

# Parse dates
dates <- c("2023-01-01", "2023-02-15", "2023-03-10")
parsed_dates <- ymd(dates)

# Extract components
years <- year(parsed_dates)
months <- month(parsed_dates)
days <- day(parsed_dates)

# Get current date and time
current_time <- now()
```

# Comprehensive Exercise: Data Wrangling and Transformation with `tidyverse`

## Exercise: Advanced Data Manipulation with the `mtcars` Dataset

**Objective**: Use `tidyverse` functions to perform advanced data manipulation and transformation tasks on the `mtcars` dataset.

**Steps**:
1. **Select and rename columns**.
2. **Filter rows based on multiple conditions**.
3. **Create new columns and modify existing ones**.
4. **Group data and calculate summary statistics**.
5. **Handle categorical data and perform string manipulations**.
6. **Work with date-time data**.

## Detailed Steps

```r
# Load necessary libraries
library(tidyverse)
library(lubridate)

# Load the dataset
data <- mtcars

# 1. Select and rename columns
selected_data <- data %>%
  select(mpg, cyl, hp, wt) %>%
  rename(Miles_Per_Gallon = mpg, 
         Cylinders = cyl, 
         Horsepower = hp, 
         Weight = wt)

# 2. Filter rows based on multiple conditions
filtered_data <- selected_data %>%
  filter(Miles_Per_Gallon > 20, Cylinders == 4)

# 3. Create new columns and modify existing ones
mutated_data <- filtered_data %>%
  mutate(Horsepower_per_Weight = Horsepower / Weight,
         Weight_Category = ifelse(Weight > 3, "Heavy", "Light"))

# 4. Group data and calculate summary statistics
summary_data <- mutated_data %>%
  group_by(Weight_Category) %>%
  summarise(Mean_Horsepower_per_Weight = mean(Horsepower_per_Weight),
            Count = n())

# 5. Handle categorical data and perform string manipulations
# Create a factor variable
mutated_data$Weight_Category <- factor(mutated_data$Weight_Category)

# String manipulation example
car_names <- rownames(data)
car_name_lengths <- str_length(car_names)
car_name_upper <- str_to_upper(car_names)

# 6. Work with date-time data
# Create a sample date column
mutated_data <- mutated_data %>%
  mutate(Sample_Date = ymd("2023-05-01") + days(1:n()))

# Extract components of dates
mutated_data <- mutated_data %>%
  mutate(Year = year(Sample_Date),
         Month = month(Sample_Date),
         Day = day(Sample_Date))

# Comprehensive script
mtcars_analysis <- function() {
  # Load and manipulate data
  data <- mtcars
  
  # Select and rename columns
  selected_data <- data %>%
    select(mpg, cyl, hp, wt) %>%
    rename(Miles_Per_Gallon = mpg, 
           Cylinders = cyl, 
           Horsepower = hp, 
           Weight = wt)

  # Filter rows based on conditions
  filtered_data <- selected_data %>%
    filter(Miles_Per_Gallon > 20, Cylinders == 4)

  # Create and modify columns
  mutated_data <- filtered_data %>%
    mutate(Horsepower_per_Weight = Horsepower / Weight,
           Weight_Category = ifelse(Weight > 3, "Heavy", "Light"))

  # Group and summarize data
  summary_data <- mutated_data %>%
    group_by(Weight_Category) %>%
    summarise(Mean_Horsepower_per_Weight = mean(Horsepower_per_Weight),
              Count = n())

  print("Summary Data:")
  print(summary_data)

  # String manipulation
  car_names <- rownames(data)
  car_name_lengths <- str_length(car_names)
  car_name_upper <- str_to_upper(car_names)
  print("Car Names in Upper Case:")
  print(car_name_upper)

  # Work with date-time data
  mutated_data <- mutated_data %>%
    mutate(Sample_Date = ymd("2023-05-01") + days(1:n()),
           Year = year(Sample_Date),
           Month = month(Sample_Date),
           Day = day(Sample_Date))
  
  print("Mutated Data with Dates:")
  print(mutated_data)
}

# Run the comprehensive analysis
mtcars_analysis()
```

<!--chapter:end:14-DataWrangling.Rmd-->

# Advanced R Programming Techniques (`OPTIONAL`)

## Writing Functions and Loops for Automating Tasks

In advanced R programming, writing functions and loops efficiently is crucial for automating repetitive tasks and handling complex data operations.

### Function Arguments and Return Values
- **Function Arguments**: Functions can take multiple arguments, including default values, to make them more flexible and reusable.
- **Return Values**: Functions can return a single value or multiple values as a list.

**Example**:

```r
# Define a function with arguments and return value
my_function <- function(x, y = 2) {
  result <- x * y
  return(result)
}

# Call the function
my_function(5)    # Uses default value for y
my_function(5, 3) # Uses provided value for y
```

### Looping Constructs: `for`, `while`, and `apply` Family
- **`for` Loop**: Iterates over a sequence of elements.
- **`while` Loop**: Continues to execute as long as a condition is true.
- **`apply` Family**: Functions like `lapply`, `sapply`, `tapply`, and `mapply` apply a function over elements of a list or vector, offering more concise and often faster alternatives to loops.

**Example**:

```r
# for loop example
for (i in 1:5) {
  print(i)
}

# while loop example
i <- 1
while (i <= 5) {
  print(i)
  i <- i + 1
}

# apply family example
vec <- 1:5
squared <- sapply(vec, function(x) x^2)
print(squared)
```

### Avoiding Loops: Vectorization and Parallel Processing
- **Vectorization**: Writing code that operates on entire vectors or matrices at once, which is usually faster and more efficient than using loops.
- **Parallel Processing**: Using multiple cores or processors to perform computations simultaneously, which can significantly speed up data processing tasks.

**Example**:

```r
# Vectorized operation
vec <- 1:5
squared_vec <- vec^2
print(squared_vec)

# Parallel processing example using `parallel` package
library(parallel)
cl <- makeCluster(detectCores() - 1)
parSapply(cl, 1:5, function(x) x^2)
stopCluster(cl)
```

## Comprehensive Exercise: Automating Data Analysis with Functions and Loops

### Exercise: Automating Data Analysis with the `iris` Dataset

**Objective**: Write functions and use loops to automate the process of summarizing and analyzing the `iris` dataset. The exercise will include calculating summary statistics, visualizing data, and applying a machine learning model.

**Steps**:
1. **Write a function to calculate summary statistics for each species**.
2. **Use a loop to generate boxplots for each variable by species**.
3. **Write a function to train a decision tree model and evaluate its accuracy**.
4. **Combine these tasks into a comprehensive script that automates the entire process**.

### Detailed Steps

```r
# Load necessary libraries
library(ggplot2)
library(caret)
library(rpart)
library(dplyr)

# 1. Function to calculate summary statistics for each species
summary_stats <- function(data, species) {
  data %>% 
    filter(Species == species) %>%
    summarise(across(where(is.numeric), list(mean = mean, sd = sd)))
}

# 2. Loop to generate boxplots for each variable by species
plot_boxplots <- function(data) {
  variables <- colnames(data)[1:4]
  for (var in variables) {
    p <- ggplot(data, aes_string(x = "Species", y = var)) + 
      geom_boxplot() +
      ggtitle(paste("Boxplot of", var, "by Species"))
    print(p)
  }
}

# 3. Function to train a decision tree model and evaluate its accuracy
train_decision_tree <- function(data) {
  set.seed(123)
  train_index <- createDataPartition(data$Species, p = 0.8, list = FALSE)
  train_data <- data[train_index, ]
  test_data <- data[-train_index, ]
  
  model <- rpart(Species ~ ., data = train_data, method = "class")
  predictions <- predict(model, test_data, type = "class")
  accuracy <- confusionMatrix(predictions, test_data$Species)$overall['Accuracy']
  
  return(accuracy)
}

# 4. Comprehensive script
iris_analysis <- function() {
  data <- iris
  
  # Calculate summary statistics for each species
  species <- unique(data$Species)
  for (sp in species) {
    cat("\nSummary statistics for", sp, ":\n")
    print(summary_stats(data, sp))
  }
  
  # Generate boxplots
  plot_boxplots(data)
  
  # Train decision tree model and evaluate accuracy
  accuracy <- train_decision_tree(data)
  cat("\nDecision tree model accuracy:", accuracy, "\n")
}

# Run the comprehensive analysis
iris_analysis()
```

<!--chapter:end:15-AdvancedRProgramming.Rmd-->

